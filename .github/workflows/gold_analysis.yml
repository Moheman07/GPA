name: üèÜ Enhanced Gold Analysis (Lightweight)
on:
  schedule:
    - cron: '30 13 * * 1-5'  # ŸäŸàŸÖŸä
  workflow_dispatch:

env:
  NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}

jobs:
  enhanced-analysis:
    name: üß† Enhanced Gold Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 12
    permissions:
      contents: write
    
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        
    - name: üì¶ Install Lightweight Packages
      run: |
        echo "üîÑ Installing enhanced packages (lightweight)..."
        pip install --upgrade pip --quiet
        
        # Core packages
        pip install --no-cache-dir --quiet \
          yfinance>=0.2.30 \
          pandas>=2.0.0 \
          numpy>=1.24.0 \
          requests>=2.31.0 \
          scipy>=1.10.0
        
        # Lightweight sentiment analysis
        pip install --no-cache-dir --quiet \
          vaderSentiment>=3.3.2 \
          textblob>=0.17.1 \
          nltk>=3.8.1
        
        # Visualization and analysis
        pip install --no-cache-dir --quiet \
          matplotlib>=3.7.0 \
          seaborn>=0.12.0 \
          scikit-learn>=1.3.0
        
        # Download NLTK data
        python -c "
        import nltk
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('vader_lexicon', quiet=True)
            nltk.download('stopwords', quiet=True)
            print('‚úÖ NLTK data downloaded successfully')
        except Exception as e:
            print(f'‚ö†Ô∏è NLTK download issue: {e}')
        "
        
        echo "‚úÖ Enhanced packages installed successfully"

    - name: üèóÔ∏è Create Enhanced Gold Analyzer
      run: |
        cat > enhanced_analyzer.py << 'EOF'
        #!/usr/bin/env python3
        # -*- coding: utf-8 -*-
        """
        üèÜ ŸÖÿ≠ŸÑŸÑ ÿßŸÑÿ∞Ÿáÿ® ÿßŸÑŸÖÿ≠ÿ≥ŸÜ - ŸÜÿ≥ÿÆÿ© ÿÆŸÅŸäŸÅÿ© ŸàŸÇŸàŸäÿ©
        Enhanced Gold Analysis System (Lightweight but Powerful)
        """
        
        import yfinance as yf
        import pandas as pd
        import numpy as np
        import requests
        import json
        import sqlite3
        import os
        import logging
        import warnings
        from datetime import datetime, timedelta
        from typing import Dict, List, Optional, Any
        import time
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Sentiment Analysis (Lightweight)
        try:
            from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
            from textblob import TextBlob
            import nltk
            SENTIMENT_AVAILABLE = True
        except ImportError:
            SENTIMENT_AVAILABLE = False
        
        # ML for advanced analysis
        try:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.preprocessing import StandardScaler
            ML_AVAILABLE = True
        except ImportError:
            ML_AVAILABLE = False
        
        warnings.filterwarnings('ignore')
        
        # Enhanced logging setup
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('gold_analysis.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        logger = logging.getLogger(__name__)
        
        class EnhancedGoldAnalyzer:
            """ŸÖÿ≠ŸÑŸÑ ÿßŸÑÿ∞Ÿáÿ® ÿßŸÑŸÖÿ≠ÿ≥ŸÜ - ŸÜÿ≥ÿÆÿ© ÿÆŸÅŸäŸÅÿ© ŸàŸÇŸàŸäÿ©"""
            
            def __init__(self):
                self.symbols = {
                    'gold_futures': 'GC=F',
                    'gold_etf': 'GLD', 
                    'silver_etf': 'SLV',
                    'dollar_index': 'DX-Y.NYB',
                    'vix': '^VIX',
                    'treasury_10y': '^TNX',
                    'sp500': 'SPY',
                    'oil': 'CL=F',
                    'bitcoin': 'BTC-USD'
                }
                
                self.news_api_key = os.getenv('NEWS_API_KEY')
                self.db_path = 'gold_analysis_history.db'
                
                # Initialize sentiment analyzer
                self.sentiment_analyzer = SentimentIntensityAnalyzer() if SENTIMENT_AVAILABLE else None
                
                # Data storage
                self.market_data = None
                self.gold_data = None
                
                logger.info("üöÄ ÿ™ŸÖ ÿ™ŸáŸäÿ¶ÿ© ŸÖÿ≠ŸÑŸÑ ÿßŸÑÿ∞Ÿáÿ® ÿßŸÑŸÖÿ≠ÿ≥ŸÜ ÿ®ŸÜÿ¨ÿßÿ≠")
        
            def setup_database(self):
                """ÿ•ÿπÿØÿßÿØ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ŸÇÿØŸÖÿ©"""
                try:
                    conn = sqlite3.connect(self.db_path)
                    cursor = conn.cursor()
                    
                    # Main analysis table
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS gold_analysis_history (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            timestamp TEXT NOT NULL,
                            signal TEXT NOT NULL,
                            signal_strength TEXT NOT NULL,
                            total_score REAL NOT NULL,
                            gold_price REAL,
                            technical_score REAL,
                            news_sentiment_score REAL,
                            market_condition TEXT,
                            stop_loss REAL,
                            take_profit REAL,
                            rsi REAL,
                            macd_signal TEXT,
                            bb_position REAL,
                            volume_trend TEXT,
                            volatility REAL,
                            trend_strength TEXT,
                            news_articles_count INTEGER,
                            news_positive_count INTEGER,
                            news_negative_count INTEGER,
                            top_news TEXT,
                            execution_time_ms INTEGER,
                            analysis_confidence REAL,
                            market_data_points INTEGER,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    ''')
                    
                    # News details table
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS news_analysis_detail (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            analysis_id INTEGER,
                            headline TEXT,
                            source TEXT,
                            sentiment_score REAL,
                            textblob_sentiment REAL,
                            relevance_score INTEGER,
                            published_at TEXT,
                            url TEXT,
                            keywords TEXT,
                            FOREIGN KEY (analysis_id) REFERENCES gold_analysis_history (id)
                        )
                    ''')
                    
                    conn.commit()
                    conn.close()
                    logger.info("‚úÖ ÿ™ŸÖ ÿ•ÿπÿØÿßÿØ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ŸÜÿ¨ÿßÿ≠")
                    
                except Exception as e:
                    logger.error(f"‚ùå ŸÅÿ¥ŸÑ ÿ•ÿπÿØÿßÿØ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™: {e}")
        
            def fetch_market_data_robust(self) -> bool:
                """ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ≥ŸàŸÇ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿ≠ÿ≥ŸÜÿ© ŸàŸÖŸÇÿßŸàŸÖÿ© ŸÑŸÑÿ£ÿÆÿ∑ÿßÿ°"""
                logger.info("üìä ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ≥ŸàŸÇ ÿßŸÑŸÖÿ≠ÿ≥ŸÜÿ©...")
                
                try:
                    symbols_list = list(self.symbols.values())
                    
                    # Primary attempt with all symbols
                    try:
                        logger.info("üîÑ ŸÖÿ≠ÿßŸàŸÑÿ© ÿ¨ŸÑÿ® ÿ¨ŸÖŸäÿπ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™...")
                        self.market_data = yf.download(
                            symbols_list,
                            period="2y",
                            interval="1d",
                            progress=True,
                            group_by='ticker'
                        )
                        
                        if self.market_data is not None and not self.market_data.empty:
                            logger.info(f"‚úÖ ÿ™ŸÖ ÿ¨ŸÑÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ŸÜÿ¨ÿßÿ≠: {len(self.market_data)} ŸäŸàŸÖ")
                        else:
                            raise Exception("ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÅÿßÿ±ÿ∫ÿ©")
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è ŸÅÿ¥ŸÑ ÿßŸÑÿ¨ŸÑÿ® ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä: {e}")
                        
                        # Fallback to essential symbols only
                        logger.info("üîÑ ŸÖÿ≠ÿßŸàŸÑÿ© ÿ¨ŸÑÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©...")
                        essential_symbols = ['GLD', 'SLV', 'DX-Y.NYB', '^VIX', 'SPY']
                        
                        self.market_data = yf.download(
                            essential_symbols,
                            period="1y",
                            interval="1d",
                            progress=False
                        )
                        
                        if self.market_data is None or self.market_data.empty:
                            raise Exception("ŸÅÿ¥ŸÑ ŸÅŸä ÿ¨ŸÑÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©")
                    
                    # Process gold data with multiple fallbacks
                    gold_data_extracted = False
                    
                    # Try GC=F first (futures)
                    for gold_symbol in ['GC=F', 'GLD']:
                        try:
                            if hasattr(self.market_data, 'columns') and any(gold_symbol in str(col) for col in self.market_data.columns):
                                # Multi-column format
                                if ('Close', gold_symbol) in self.market_data.columns:
                                    self.gold_data = pd.DataFrame({
                                        'Open': self.market_data[('Open', gold_symbol)],
                                        'High': self.market_data[('High', gold_symbol)],
                                        'Low': self.market_data[('Low', gold_symbol)],
                                        'Close': self.market_data[('Close', gold_symbol)],
                                        'Volume': self.market_data[('Volume', gold_symbol)]
                                    }).dropna()
                                    gold_data_extracted = True
                                    logger.info(f"‚úÖ ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ∞Ÿáÿ® ŸÖŸÜ {gold_symbol}")
                                    break
                                    
                            # Try direct column access
                            elif gold_symbol in self.market_data.columns:
                                self.gold_data = self.market_data[gold_symbol].dropna()
                                gold_data_extracted = True
                                logger.info(f"‚úÖ ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ∞Ÿáÿ® ŸÖŸÜ {gold_symbol} (ŸÖÿ®ÿßÿ¥ÿ±)")
                                break
                                
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è ŸÅÿ¥ŸÑ ŸÅŸä ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ {gold_symbol}: {e}")
                            continue
                    
                    # Last resort: download gold data separately
                    if not gold_data_extracted:
                        logger.info("üîÑ ŸÖÿ≠ÿßŸàŸÑÿ© ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ∞Ÿáÿ® ŸÖŸÜŸÅÿµŸÑÿ©...")
                        for symbol in ['GLD', 'GC=F']:
                            try:
                                gold_data_single = yf.download(symbol, period='1y', progress=False)
                                if not gold_data_single.empty:
                                    self.gold_data = gold_data_single
                                    gold_data_extracted = True
                                    logger.info(f"‚úÖ ÿ™ŸÖ ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ∞Ÿáÿ® ŸÖŸÜŸÅÿµŸÑÿ©: {symbol}")
                                    break
                            except:
                                continue
                    
                    if not gold_data_extracted or self.gold_data is None or self.gold_data.empty:
                        raise Exception("ŸÅÿ¥ŸÑ ŸÅŸä ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ∞Ÿáÿ®")
                    
                    logger.info(f"‚úÖ ÿ™ŸÖ ÿ¨ŸÑÿ® {len(self.gold_data)} ŸäŸàŸÖ ŸÖŸÜ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ∞Ÿáÿ® ÿ®ŸÜÿ¨ÿßÿ≠")
                    return True
                    
                except Exception as e:
                    logger.error(f"‚ùå ŸÅÿ¥ŸÑ ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ≥ŸàŸÇ: {e}")
                    return False
        
            def calculate_comprehensive_technical_indicators(self):
                """ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ© ÿßŸÑÿ¥ÿßŸÖŸÑÿ©"""
                logger.info("üìà ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ© ÿßŸÑŸÖÿ™ŸÇÿØŸÖÿ©...")
                
                if self.gold_data is None or self.gold_data.empty:
                    logger.error("‚ùå ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ®ŸäÿßŸÜÿßÿ™ ÿ∞Ÿáÿ® ŸÑŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÅŸÜŸä")
                    return None
                
                try:
                    df = self.gold_data.copy()
                    
                    # Ensure we have the basic OHLCV columns
                    required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
                    missing_cols = [col for col in required_cols if col not in df.columns]
                    
                    if missing_cols:
                        # If we only have Close prices, create synthetic OHLV
                        if 'Close' in df.columns:
                            df['Open'] = df['Close'].shift(1).fillna(df['Close'])
                            df['High'] = df['Close'] * 1.01  # Synthetic high
                            df['Low'] = df['Close'] * 0.99   # Synthetic low
                            df['Volume'] = 1000000  # Synthetic volume
                        else:
                            raise Exception("ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑÿß ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ£ÿ≥ÿπÿßÿ± ÿßŸÑÿ•ÿ∫ŸÑÿßŸÇ")
                    
                    # Moving Averages
                    for period in [5, 10, 20, 50, 100, 200]:
                        df[f'SMA_{period}'] = df['Close'].rolling(period, min_periods=1).mean()
                        df[f'EMA_{period}'] = df['Close'].ewm(span=period).mean()
                    
                    # RSI Calculation
                    def calculate_rsi(prices, period=14):
                        delta = prices.diff()
                        gain = delta.where(delta > 0, 0).rolling(period, min_periods=1).mean()
                        loss = (-delta.where(delta < 0, 0)).rolling(period, min_periods=1).mean()
                        rs = gain / loss.replace(0, np.inf)
                        return 100 - (100 / (1 + rs))
                    
                    df['RSI'] = calculate_rsi(df['Close'])
                    
                    # MACD
                    df['MACD'] = df['EMA_12'] - df['EMA_26']
                    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()
                    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']
                    
                    # Bollinger Bands
                    df['BB_Middle'] = df['Close'].rolling(20, min_periods=1).mean()
                    df['BB_Std'] = df['Close'].rolling(20, min_periods=1).std()
                    df['BB_Upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)
                    df['BB_Lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)
                    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']
                    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])
                    
                    # Stochastic Oscillator
                    def calculate_stochastic(high, low, close, k_period=14, d_period=3):
                        lowest_low = low.rolling(k_period, min_periods=1).min()
                        highest_high = high.rolling(k_period, min_periods=1).max()
                        k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))
                        d_percent = k_percent.rolling(d_period, min_periods=1).mean()
                        return k_percent, d_percent
                    
                    df['Stoch_K'], df['Stoch_D'] = calculate_stochastic(df['High'], df['Low'], df['Close'])
                    
                    # Williams %R
                    period = 14
                    highest_high = df['High'].rolling(period, min_periods=1).max()
                    lowest_low = df['Low'].rolling(period, min_periods=1).min()
                    df['Williams_R'] = -100 * ((highest_high - df['Close']) / (highest_high - lowest_low))
                    
                    # Average True Range (ATR)
                    def calculate_atr(high, low, close, period=14):
                        tr1 = high - low
                        tr2 = abs(high - close.shift(1))
                        tr3 = abs(low - close.shift(1))
                        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
                        return tr.rolling(period, min_periods=1).mean()
                    
                    df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'])
                    
                    # Volume Analysis
                    df['Volume_SMA'] = df['Volume'].rolling(20, min_periods=1).mean()
                    df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']
                    
                    # On Balance Volume (OBV)
                    df['OBV'] = (df['Volume'] * np.where(df['Close'] > df['Close'].shift(1), 1, -1)).cumsum()
                    
                    # Momentum indicators
                    df['ROC'] = ((df['Close'] / df['Close'].shift(12)) - 1) * 100  # 12-day Rate of Change
                    df['Momentum'] = df['Close'] / df['Close'].shift(10)
                    
                    # Volatility
                    df['Volatility'] = df['Close'].pct_change().rolling(20, min_periods=1).std() * np.sqrt(252)
                    
                    # Support and Resistance levels
                    df['Support'] = df['Low'].rolling(20, min_periods=1).min()
                    df['Resistance'] = df['High'].rolling(20, min_periods=1).max()
                    
                    # Advanced patterns
                    df['Higher_High'] = (df['High'] > df['High'].shift(1)) & (df['High'].shift(1) > df['High'].shift(2))
                    df['Lower_Low'] = (df['Low'] < df['Low'].shift(1)) & (df['Low'].shift(1) < df['Low'].shift(2))
                    
                    # Clean data
                    df = df.dropna()
                    
                    if df.empty:
                        raise Exception("ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ£ÿµÿ®ÿ≠ÿ™ ŸÅÿßÿ±ÿ∫ÿ© ÿ®ÿπÿØ ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™")
                    
                    # Update data
                    self.gold_data = df
                    
                    logger.info(f"‚úÖ ÿ™ŸÖ ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ© - ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÜÿ∏ŸäŸÅÿ©: {len(df)} ÿµŸÅ")
                    return df
                    
                except Exception as e:
                    logger.error(f"‚ùå ŸÅÿ¥ŸÑ ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ©: {e}")
                    return None
        
            def analyze_news_with_enhanced_sentiment(self) -> Dict[str, Any]:
                """ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ≠ÿ≥ŸÜ ŸÖÿπ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ¥ÿßÿπÿ± ÿßŸÑŸÖÿ™ŸÇÿØŸÖ"""
                logger.info("üì∞ ÿ®ÿØÿ° ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ™ÿÆÿµÿµ...")
                
                if not self.news_api_key:
                    return {
                        'status': 'no_api_key',
                        'sentiment_score': 0,
                        'articles_count': 0,
                        'top_articles': [],
                        'summary': 'ŸÖŸÅÿ™ÿßÿ≠ API ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ±'
                    }
                
                try:
                    # Enhanced queries for gold-related news
                    specialized_queries = [
                        'gold OR XAU OR bullion OR "precious metals" OR "gold price"',
                        '"interest rates" OR "federal reserve" OR "monetary policy" OR inflation',
                        '"dollar index" OR DXY OR "currency strength" OR "dollar weakness"', 
                        'geopolitical OR "safe haven" OR crisis OR "market volatility" OR recession',
                        '"central bank" OR "quantitative easing" OR "economic uncertainty"'
                    ]
                    
                    all_articles = []
                    
                    # Fetch news with parallel processing
                    def fetch_news_for_query(query):
                        try:
                            url = f"https://newsapi.org/v2/everything?q={query}&language=en&sortBy=publishedAt&pageSize=40&from={(datetime.now() - timedelta(days=3)).date()}&apiKey={self.news_api_key}"
                            response = requests.get(url, timeout=15)
                            if response.status_code == 200:
                                articles = response.json().get('articles', [])
                                return articles
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è ÿÆÿ∑ÿ£ ŸÅŸä ÿ¨ŸÑÿ® ÿßŸÑÿ£ÿÆÿ®ÿßÿ±: {e}")
                        return []
                    
                    # Parallel news fetching
                    with ThreadPoolExecutor(max_workers=3) as executor:
                        future_to_query = {executor.submit(fetch_news_for_query, query): query for query in specialized_queries}
                        
                        for future in as_completed(future_to_query, timeout=30):
                            query = future_to_query[future]
                            try:
                                articles = future.result()
                                all_articles.extend(articles)
                                logger.info(f"üì• ÿ¨ŸÑÿ® {len(articles)} ŸÖŸÇÿßŸÑ ŸÖŸÜ ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ: {query[:50]}...")
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è ŸÅÿ¥ŸÑ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ: {e}")
                    
                    if not all_articles:
                        return {
                            'status': 'no_articles',
                            'sentiment_score': 0,
                            'articles_count': 0,
                            'top_articles': [],
                            'summary': 'ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÖŸÇÿßŸÑÿßÿ™'
                        }
                    
                    # Remove duplicates
                    unique_articles = []
                    seen_urls = set()
                    
                    for article in all_articles:
                        url = article.get('url', '')
                        title = article.get('title', '')
                        
                        if url not in seen_urls and len(title) > 15:
                            seen_urls.add(url)
                            unique_articles.append(article)
                    
                    logger.info(f"üîç ÿ™ŸÖ ÿ¨ŸÑÿ® {len(unique_articles)} ŸÖŸÇÿßŸÑÿßŸã ŸÅÿ±ŸäÿØÿßŸã")
                    
                    # Enhanced relevance scoring
                    gold_keywords = {
                        'direct_gold': ['gold', 'xau', 'bullion', 'precious metal'],
                        'monetary_policy': ['fed', 'federal reserve', 'interest rate', 'monetary policy', 'inflation'],
                        'currency': ['dollar', 'dxy', 'currency', 'dollar index'],
                        'market_sentiment': ['safe haven', 'geopolitical', 'crisis', 'uncertainty', 'volatility'],
                        'economic': ['recession', 'economic', 'gdp', 'unemployment', 'central bank']
                    }
                    
                    relevant_articles = []
                    
                    for article in unique_articles:
                        title = article.get('title', '').lower()
                        description = article.get('description', '').lower() if article.get('description') else ''
                        content = f"{title} {description}"
                        
                        # Calculate relevance score
                        relevance_details = {}
                        total_relevance = 0
                        
                        for category, keywords in gold_keywords.items():
                            category_score = sum(1 for keyword in keywords if keyword in content)
                            if category_score > 0:
                                relevance_details[category] = category_score
                                total_relevance += category_score
                        
                        # Only include highly relevant articles
                        if total_relevance >= 2:
                            article['relevance_score'] = total_relevance
                            article['relevance_details'] = relevance_details
                            relevant_articles.append(article)
                    
                    logger.info(f"üéØ ÿ™ŸÖ ÿßÿÆÿ™Ÿäÿßÿ± {len(relevant_articles)} ŸÖŸÇÿßŸÑÿßŸã ÿ∞ÿß ÿµŸÑÿ© ÿπÿßŸÑŸäÿ© ÿ®ÿßŸÑÿ∞Ÿáÿ®")
                    
                    if not relevant_articles:
                        return {
                            'status': 'no_relevant_articles',
                            'sentiment_score': 0,
                            'articles_count': 0,
                            'top_articles': [],
                            'summary': 'ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖŸÇÿßŸÑÿßÿ™ ÿ∞ÿßÿ™ ÿµŸÑÿ©'
                        }
                    
                    # Enhanced sentiment analysis
                    analyzed_articles = []
                    sentiment_scores = []
                    
                    # Keywords for sentiment analysis
                    positive_keywords = [
                        'surge', 'rise', 'gain', 'bull', 'up', 'rally', 'climb', 'soar',
                        'strong', 'boost', 'support', 'optimism', 'confidence', 'recovery'
                    ]
                    
                    negative_keywords = [
                        'fall', 'drop', 'decline', 'bear', 'down', 'crash', 'plunge', 'slump',
                        'weak', 'pressure', 'concern', 'fear', 'uncertainty', 'risk'
                    ]
                    
                    for article in relevant_articles[:60]:  # Analyze top 60 articles
                        try:
                            title = article.get('title', '')
                            description = article.get('description', '') or ''
                            text = f"{title}. {description}"
                            
                            # VADER sentiment analysis
                            vader_score = 0
                            if self.sentiment_analyzer:
                                vader_result = self.sentiment_analyzer.polarity_scores(text)
                                vader_score = vader_result['compound']
                            
                            # TextBlob sentiment
                            textblob_score = 0
                            try:
                                blob = TextBlob(text)
                                textblob_score = blob.sentiment.polarity
                            except:
                                textblob_score = vader_score
                            
                            # Keyword-based sentiment
                            text_lower = text.lower()
                            positive_count = sum(1 for word in positive_keywords if word in text_lower)
                            negative_count = sum(1 for word in negative_keywords if word in text_lower)
                            
                            keyword_score = 0
                            if positive_count > 0 or negative_count > 0:
                                keyword_score = (positive_count - negative_count) / max(positive_count + negative_count, 1)
                            
                            # Combine sentiment scores
                            combined_score = (vader_score * 0.4 + textblob_score * 0.4 + keyword_score * 0.2)
                            
                            # Weight by relevance
                            relevance_weight = min(article['relevance_score'] / 6, 1.0)
                            final_sentiment = combined_score * relevance_weight
                            
                            # Extract key information
                            source_name = article.get('source', {}).get('name', 'Unknown')
                            published_at = article.get('publishedAt', '')
                            
                            analyzed_article = {
                                'title': title[:150],
                                'source': source_name[:50],
                                'published_at': published_at,
                                'url': article.get('url', '')[:300],
                                'relevance_score': article['relevance_score'],
                                'relevance_details': article['relevance_details'],
                                'vader_sentiment': round(vader_score, 4),
                                'textblob_sentiment': round(textblob_score, 4),
                                'keyword_sentiment': round(keyword_score, 4),
                                'combined_sentiment': round(combined_score, 4),
                                'final_sentiment': round(final_sentiment, 4),
                                'sentiment_label': self._classify_sentiment(final_sentiment),
                                'keywords_found': {
                                    'positive': [word for word in positive_keywords if word in text_lower],
                                    'negative': [word for word in negative_keywords if word in text_lower]
                                }
                            }
                            
                            analyzed_articles.append(analyzed_article)
                            sentiment_scores.append(final_sentiment)
                            
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖŸÇÿßŸÑ: {e}")
                            continue
                    
                    if not sentiment_scores:
                        return {
                            'status': 'analysis_failed',
                            'sentiment_score': 0,
                            'articles_count': 0,
                            'top_articles': [],
                            'summary': 'ŸÅÿ¥ŸÑ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖŸÇÿßŸÑÿßÿ™'
                        }
                    
                    # Calculate overall sentiment metrics
                    overall_sentiment = np.mean(sentiment_scores)
                    sentiment_std = np.std(sentiment_scores)
                    sentiment_median = np.median(sentiment_scores)
                    
                    # Count sentiment distribution
                    positive_articles = len([s for s in sentiment_scores if s > 0.1])
                    negative_articles = len([s for s in sentiment_scores if s < -0.1])
                    neutral_articles = len(sentiment_scores) - positive_articles - negative_articles
                    
                    # Sort articles by impact (relevance + absolute sentiment)
                    analyzed_articles.sort(
                        key=lambda x: (x['relevance_score'] * 2 + abs(x['final_sentiment'])),
                        reverse=True
                    )
                    
                    # Calculate confidence based on consistency and volume
                    volume_confidence = min(1.0, len(analyzed_articles) / 30)
                    consistency_confidence = 1 - min(sentiment_std, 1.0)
                    overall_confidence = (volume_confidence * 0.6 + consistency_confidence * 0.4)
                    
                    logger.info(f"üìä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ŸÖŸÉÿ™ŸÖŸÑ: ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ© {overall_sentiment:.3f}")
                    
                    return {
                        'status': 'success',
                        'sentiment_score': round(overall_sentiment, 4),
                        'sentiment_median': round(sentiment_median, 4),
                        'sentiment_std': round(sentiment_std, 4),
                        'articles_count': len(analyzed_articles),
                        'positive_count': positive_articles,
                        'negative_count': negative_articles,
                        'neutral_count': neutral_articles,
                        'confidence': round(overall_confidence, 3),
                        'top_articles': analyzed_articles[:15],
                        'summary': f"ÿ™ŸÖ ÿ™ÿ≠ŸÑŸäŸÑ {len(analyzed_articles)} ŸÖŸÇÿßŸÑÿßŸã: {positive_articles} ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿå {negative_articles} ÿ≥ŸÑÿ®Ÿäÿå {neutral_articles} ŸÖÿ≠ÿßŸäÿØ",
                        'sentiment_distribution': {
                            'very_positive': len([s for s in sentiment_scores if s > 0.5]),
                            'positive': len([s for s in sentiment_scores if 0.1 < s <= 0.5]),
                            'neutral': neutral_articles,
                            'negative': len([s for s in sentiment_scores if -0.5 <= s < -0.1]),
                            'very_negative': len([s for s in sentiment_scores if s < -0.5])
                        }
                    }
                    
                except Exception as e:
                    logger.error(f"‚ùå ŸÅÿ¥ŸÑ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ£ÿÆÿ®ÿßÿ±: {e}")
                    return {
                        'status': 'error',
                        'error': str(e),
                        'sentiment_score': 0,
                        'articles_count': 0,
                        'top_articles': [],
                        'summary': f'ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ: {str(e)}'
                    }
        
            def _classify_sentiment(self, score: float) -> str:
                """ÿ™ÿµŸÜŸäŸÅ ÿØÿ±ÿ¨ÿ© ÿßŸÑŸÖÿ¥ÿßÿπÿ±"""
                if score >= 0.5:
                    return 'very_positive'
                elif score >= 0.2:
                    return 'positive'
                elif score >= 0.05:
                    return 'slightly_positive'
                elif score <= -0.5:
                    return 'very_negative'
                elif score <= -0.2:
                    return 'negative'
                elif score <= -0.05:
                    return 'slightly_negative'
                else:
                    return 'neutral'
        
            def generate_comprehensive_trading_signal(self, technical_data, news_data) -> Dict[str, Any]:
                """ÿ™ŸàŸÑŸäÿØ ÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑÿ™ÿØÿßŸàŸÑ ÿßŸÑÿ¥ÿßŸÖŸÑÿ© ÿßŸÑŸÖÿ≠ÿ≥ŸÜÿ©"""
                try:
                    if technical_data is None or technical_data.empty:
                        return {
                            'signal': 'Hold',
                            'strength': 'Neutral',
                            'total_score': 0,
                            'confidence': 0
                        }
                    
                    latest = technical_data.iloc[-1]
                    
                    # Technical Analysis Score (70% weight)
                    technical_components = {}
                    
                    # 1. Trend Analysis (35% of technical)
                    trend_score = 0
                    current_price = latest['Close']
                    
                    # Moving average analysis
                    if current_price > latest['SMA_200']: trend_score += 4
                    if current_price > latest['SMA_50']: trend_score += 3
                    if current_price > latest['SMA_20']: trend_score += 2
                    if latest['SMA_20'] > latest['SMA_50']: trend_score += 2
                    if latest['SMA_50'] > latest['SMA_200']: trend_score += 1
                    
                    technical_components['trend'] = (trend_score / 12) * 2 - 1  # Normalize to -1 to +1
                    
                    # 2. Momentum Analysis (35% of technical)
                    momentum_score = 0
                    rsi = latest['RSI']
                    macd = latest['MACD']
                    macd_signal = latest['MACD_Signal']
                    stoch_k = latest['Stoch_K']
                    
                    # MACD analysis
                    if macd > macd_signal: momentum_score += 3
                    if latest['MACD_Histogram'] > 0: momentum_score += 1
                    
                    # RSI analysis
                    if 40 <= rsi <= 60: momentum_score += 2  # Healthy momentum
                    elif 30 <= rsi < 40: momentum_score += 3  # Oversold opportunity
                    elif 60 < rsi <= 70: momentum_score += 1  # Still bullish
                    elif rsi < 30: momentum_score += 4  # Severely oversold
                    elif rsi > 70: momentum_score -= 2  # Overbought warning
                    
                    # Stochastic
                    if stoch_k < 20: momentum_score += 1  # Oversold
                    elif stoch_k > 80: momentum_score -= 1  # Overbought
                    
                    technical_components['momentum'] = max(-1, min(1, (momentum_score - 3) / 6))
                    
                    # 3. Volatility & Support/Resistance (20% of technical)
                    volatility_score = 0
                    bb_position = latest['BB_Position']
                    atr = latest['ATR']
                    volatility = latest['Volatility']
                    
                    # Bollinger Bands analysis
                    if bb_position < 0.2: volatility_score += 2  # Near lower band - potential bounce
                    elif bb_position > 0.8: volatility_score -= 1  # Near upper band - resistance
                    elif 0.4 <= bb_position <= 0.6: volatility_score += 0.5  # Healthy middle range
                    
                    # Volatility analysis
                    if volatility > 0.30: volatility_score += 1  # High volatility favors gold
                    elif volatility < 0.15: volatility_score -= 0.5  # Low volatility
                    
                    technical_components['volatility'] = max(-1, min(1, volatility_score / 2))
                    
                    # 4. Volume Analysis (10% of technical)
                    volume_score = 0
                    volume_ratio = latest['Volume_Ratio']
                    
                    if volume_ratio > 1.5: volume_score += 1  # High volume confirms moves
                    elif volume_ratio > 1.2: volume_score += 0.5
                    elif volume_ratio < 0.7: volume_score -= 0.5  # Low volume is concerning
                    
                    technical_components['volume'] = max(-1, min(1, volume_score))
                    
                    # Calculate weighted technical score
                    technical_weights = {'trend': 0.35, 'momentum': 0.35, 'volatility': 0.20, 'volume': 0.10}
                    technical_score = sum(technical_components[comp] * weight for comp, weight in technical_weights.items())
                    
                    # News Sentiment Score (30% weight)
                    news_components = {}
                    news_score = 0
                    
                    if news_data.get('status') == 'success':
                        raw_sentiment = news_data.get('sentiment_score', 0)
                        confidence = news_data.get('confidence', 0)
                        article_count = news_data.get('articles_count', 0)
                        
                        # Weight by confidence and article volume
                        volume_weight = min(1.0, article_count / 25)
                        news_score = raw_sentiment * confidence * volume_weight
                        
                        news_components['sentiment'] = raw_sentiment
                        news_components['confidence'] = confidence
                        news_components['volume_weight'] = volume_weight
                    
                    # Final weighted score
                    total_score = (technical_score * 0.70) + (news_score * 0.30)
                    
                    # Determine signal and strength
                    if total_score >= 0.7:
                        signal, strength = 'Strong Buy', 'Very Strong'
                    elif total_score >= 0.4:
                        signal, strength = 'Buy', 'Strong'
                    elif total_score >= 0.15:
                        signal, strength = 'Buy', 'Moderate'
                    elif total_score <= -0.7:
                        signal, strength = 'Strong Sell', 'Very Strong'
                    elif total_score <= -0.4:
                        signal, strength = 'Sell', 'Strong'
                    elif total_score <= -0.15:
                        signal, strength = 'Sell', 'Moderate'
                    else:
                        signal, strength = 'Hold', 'Neutral'
                    
                    # Calculate confidence
                    component_consistency = 1 - np.std(list(technical_components.values()))
                    signal_confidence = min(1.0, abs(total_score) * component_consistency)
                    
                    # Risk management calculations
                    atr_current = latest['ATR']
                    
                    if 'Buy' in signal:
                        stop_loss = current_price - (2.5 * atr_current)
                        take_profit = current_price + (4.0 * atr_current)
                    elif 'Sell' in signal:
                        stop_loss = current_price + (2.5 * atr_current)
                        take_profit = current_price - (4.0 * atr_current)
                    else:
                        stop_loss = current_price - (1.5 * atr_current)
                        take_profit = current_price + (2.5 * atr_current)
                    
                    # Market condition assessment
                    market_condition = self._assess_enhanced_market_condition(technical_data)
                    
                    return {
                        'signal': signal,
                        'strength': strength,
                        'total_score': round(total_score, 4),
                        'technical_score': round(technical_score, 4),
                        'news_score': round(news_score, 4),
                        'confidence': round(signal_confidence, 3),
                        
                        # Market data
                        'current_price': round(current_price, 2),
                        'stop_loss': round(stop_loss, 2),
                        'take_profit': round(take_profit, 2),
                        
                        # Technical indicators
                        'rsi': round(rsi, 1),
                        'macd_signal': 'Bullish' if macd > macd_signal else 'Bearish',
                        'bb_position': round(bb_position, 3),
                        'volume_trend': 'High' if latest['Volume_Ratio'] > 1.5 else 'Normal' if latest['Volume_Ratio'] > 0.8 else 'Low',
                        'volatility': round(latest['Volatility'], 3),
                        'atr': round(atr_current, 2),
                        
                        # Advanced metrics
                        'market_condition': market_condition,
                        'trend_strength': self._assess_trend_strength(technical_components['trend']),
                        'momentum_status': self._assess_momentum_status(technical_components['momentum']),
                        
                        # Component breakdown
                        'technical_components': {k: round(v, 3) for k, v in technical_components.items()},
                        'technical_weights': technical_weights,
                        'news_components': {k: round(v, 3) for k, v in news_components.items()},
                        
                        # Risk metrics
                        'risk_reward_ratio': round(abs(take_profit - current_price) / abs(current_price - stop_loss), 2) if stop_loss != current_price else 0,
                        'position_size_suggestion': self._calculate_position_size(signal_confidence, latest['Volatility']),
                        'max_risk_per_trade': 2.0  # Maximum 2% risk per trade
                    }
                    
                except Exception as e:
                    logger.error(f"‚ùå ŸÅÿ¥ŸÑ ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ©: {e}")
                    return {
                        'signal': 'Hold',
                        'strength': 'Neutral',
                        'total_score': 0,
                        'confidence': 0,
                        'error': str(e)
                    }
        
            def _assess_enhanced_market_condition(self, technical_data) -> str:
                """ÿ™ŸÇŸäŸäŸÖ ÿ≠ÿßŸÑÿ© ÿßŸÑÿ≥ŸàŸÇ ÿßŸÑŸÖÿ≠ÿ≥ŸÜÿ©"""
                try:
                    latest = technical_data.iloc[-1]
                    recent = technical_data.tail(5)
                    
                    # Volatility analysis
                    current_volatility = latest['Volatility']
                    avg_volatility = recent['Volatility'].mean()
                    
                    # Trend analysis
                    sma_20 = latest['SMA_20']
                    sma_50 = latest['SMA_50']
                    current_price = latest['Close']
                    
                    # Volume analysis
                    avg_volume_ratio = recent['Volume_Ratio'].mean()
                    
                    if current_volatility > 0.35:
                        return 'high_volatility'
                    elif current_volatility < 0.12:
                        return 'low_volatility'
                    elif current_price > sma_50 and sma_20 > sma_50 and avg_volume_ratio > 1.2:
                        return 'strong_uptrend'
                    elif current_price < sma_50 and sma_20 < sma_50 and avg_volume_ratio > 1.2:
                        return 'strong_downtrend'
                    elif abs(current_price - sma_50) / sma_50 < 0.02:
                        return 'consolidation'
                    else:
                        return 'normal'
                        
                except:
                    return 'normal'
        
            def _assess_trend_strength(self, trend_score: float) -> str:
                """ÿ™ŸÇŸäŸäŸÖ ŸÇŸàÿ© ÿßŸÑÿßÿ™ÿ¨ÿßŸá"""
                if trend_score >= 0.7:
                    return 'Very Strong Bullish'
                elif trend_score >= 0.3:
                    return 'Strong Bullish'
                elif trend_score >= 0.1:
                    return 'Moderate Bullish'
                elif trend_score <= -0.7:
                    return 'Very Strong Bearish'
                elif trend_score <= -0.3:
                    return 'Strong Bearish'
                elif trend_score <= -0.1:
                    return 'Moderate Bearish'
                else:
                    return 'Neutral'
        
            def _assess_momentum_status(self, momentum_score: float) -> str:
                """ÿ™ŸÇŸäŸäŸÖ ÿ≠ÿßŸÑÿ© ÿßŸÑÿ≤ÿÆŸÖ"""
                if momentum_score >= 0.6:
                    return 'Strong Positive'
                elif momentum_score >= 0.2:
                    return 'Positive'
                elif momentum_score <= -0.6:
                    return 'Strong Negative'
                elif momentum_score <= -0.2:
                    return 'Negative'
                else:
                    return 'Neutral'
        
            def _calculate_position_size(self, confidence: float, volatility: float) -> float:
                """ÿ≠ÿ≥ÿßÿ® ÿ≠ÿ¨ŸÖ ÿßŸÑŸÖÿ±ŸÉÿ≤ ÿßŸÑŸÖŸÇÿ™ÿ±ÿ≠"""
                base_size = 10.0  # 10% base position
                confidence_multiplier = confidence
                volatility_adjustment = max(0.5, min(1.5, 0.20 / volatility))
                
                suggested_size = base_size * confidence_multiplier * volatility_adjustment
                return round(min(25.0, suggested_size), 1)  # Max 25% position
        
            def save_enhanced_analysis_results(self, signal_data, technical_data, news_data, execution_time) -> int:
                """ÿ≠ŸÅÿ∏ ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ≠ÿ≥ŸÜ"""
                try:
                    conn = sqlite3.connect(self.db_path)
                    cursor = conn.cursor()
                    
                    # Save main analysis
                    cursor.execute('''
                        INSERT INTO gold_analysis_history (
                            timestamp, signal, signal_strength, total_score, gold_price,
                            technical_score, news_sentiment_score, market_condition,
                            stop_loss, take_profit, rsi, macd_signal, bb_position,
                            volume_trend, volatility, trend_strength,
                            news_articles_count, news_positive_count, news_negative_count,
                            top_news, execution_time_ms, analysis_confidence, market_data_points
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        datetime.now().isoformat(),
                        signal_data.get('signal', 'Hold'),
                        signal_data.get('strength', 'Neutral'),
                        signal_data.get('total_score', 0),
                        signal_data.get('current_price', 0),
                        signal_data.get('technical_score', 0),
                        signal_data.get('news_score', 0),
                        signal_data.get('market_condition', 'normal'),
                        signal_data.get('stop_loss', 0),
                        signal_data.get('take_profit', 0),
                        signal_data.get('rsi', 50),
                        signal_data.get('macd_signal', 'Neutral'),
                        signal_data.get('bb_position', 0.5),
                        signal_data.get('volume_trend', 'Normal'),
                        signal_data.get('volatility', 0.2),
                        signal_data.get('trend_strength', 'Neutral'),
                        news_data.get('articles_count', 0),
                        news_data.get('positive_count', 0),
                        news_data.get('negative_count', 0),
                        json.dumps([article['title'] for article in news_data.get('top_articles', [])[:5]], ensure_ascii=False),
                        execution_time,
                        signal_data.get('confidence', 0),
                        len(technical_data) if technical_data is not None else 0
                    ))
                    
                    analysis_id = cursor.lastrowid
                    
                    # Save detailed news analysis
                    for article in news_data.get('top_articles', [])[:15]:
                        cursor.execute('''
                            INSERT INTO news_analysis_detail (
                                analysis_id, headline, source, sentiment_score,
                                textblob_sentiment, relevance_score, published_at, url, keywords
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            analysis_id,
                            article.get('title', '')[:400],
                            article.get('source', '')[:100],
                            article.get('vader_sentiment', 0),
                            article.get('textblob_sentiment', 0),
                            article.get('relevance_score', 0),
                            article.get('published_at', ''),
                            article.get('url', '')[:400],
                            json.dumps(article.get('keywords_found', {}), ensure_ascii=False)
                        ))
                    
                    conn.commit()
                    conn.close()
                    
                    logger.info(f"üíæ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ŸÅŸä ÿßŸÑÿ≥ÿ¨ŸÑ ÿßŸÑÿ™ÿßÿ±ŸäÿÆŸä - ID: {analysis_id}")
                    return analysis_id
                    
                except Exception as e:
                    logger.error(f"‚ùå ŸÅÿ¥ŸÑ ÿ≠ŸÅÿ∏ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨: {e}")
                    return -1
        
            def run_enhanced_comprehensive_analysis(self):
                """ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ¥ÿßŸÖŸÑ ÿßŸÑŸÖÿ≠ÿ≥ŸÜ"""
                start_time = time.time()
                logger.info("üöÄ ÿ®ÿØÿ° ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ¥ÿßŸÖŸÑ ŸÑŸÑÿ∞Ÿáÿ®...")
                
                try:
                    # Setup database
                    self.setup_database()
                    
                    # Fetch market data with robust error handling
                    logger.info("üìä ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ≥ŸàŸÇ...")
                    if not self.fetch_market_data_robust():
                        raise Exception("ŸÅÿ¥ŸÑ ŸÅŸä ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ≥ŸàŸÇ")
                    
                    # Calculate comprehensive technical indicators
                    logger.info("üìà ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ© ÿßŸÑÿ¥ÿßŸÖŸÑÿ©...")
                    technical_data = self.calculate_comprehensive_technical_indicators()
                    if technical_data is None:
                        raise Exception("ŸÅÿ¥ŸÑ ŸÅŸä ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ©")
                    
                    # Enhanced news analysis
                    logger.info("üì∞ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ≠ÿ≥ŸÜ...")
                    news_data = self.analyze_news_with_enhanced_sentiment()
                    
                    # Generate comprehensive trading signal
                    logger.info("üéØ ÿ™ŸàŸÑŸäÿØ ÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑÿ™ÿØÿßŸàŸÑ ÿßŸÑÿ¥ÿßŸÖŸÑÿ©...")
                    signal_data = self.generate_comprehensive_trading_signal(technical_data, news_data)
                    
                    # Calculate execution time
                    execution_time = int((time.time() - start_time) * 1000)
                    
                    # Save results to database
                    analysis_id = self.save_enhanced_analysis_results(signal_data, technical_data, news_data, execution_time)
                    
                    # Create comprehensive final report
                    final_report = {
                        'timestamp': datetime.now().isoformat(),
                        'analysis_id': analysis_id,
                        'execution_time_ms': execution_time,
                        'status': 'success',
                        'analysis_version': 'enhanced_v2.0',
                        
                        # Core signal
                        'signal': {
                            'primary_signal': signal_data.get('signal', 'Hold'),
                            'strength': signal_data.get('strength', 'Neutral'),
                            'total_score': signal_data.get('total_score', 0),
                            'confidence': signal_data.get('confidence', 0),
                            'technical_score': signal_data.get('technical_score', 0),
                            'news_score': signal_data.get('news_score', 0)
                        },
                        
                        # Technical analysis
                        'technical_analysis': {
                            'current_price': signal_data.get('current_price', 0),
                            'rsi': signal_data.get('rsi', 50),
                            'macd_signal': signal_data.get('macd_signal', 'Neutral'),
                            'bb_position': signal_data.get('bb_position', 0.5),
                            'volume_trend': signal_data.get('volume_trend', 'Normal'),
                            'volatility': signal_data.get('volatility', 0.2),
                            'atr': signal_data.get('atr', 0),
                            'market_condition': signal_data.get('market_condition', 'normal'),
                            'trend_strength': signal_data.get('trend_strength', 'Neutral'),
                            'momentum_status': signal_data.get('momentum_status', 'Neutral'),
                            'data_points': len(technical_data)
                        },
                        
                        # News analysis
                        'news_analysis': {
                            'status': news_data.get('status', 'unknown'),
                            'sentiment_score': news_data.get('sentiment_score', 0),
                            'articles_count': news_data.get('articles_count', 0),
                            'positive_count': news_data.get('positive_count', 0),
                            'negative_count': news_data.get('negative_count', 0),
                            'neutral_count': news_data.get('neutral_count', 0),
                            'confidence': news_data.get('confidence', 0),
                            'summary': news_data.get('summary', 'ŸÑÿß ŸäŸàÿ¨ÿØ ÿ™ŸÑÿÆŸäÿµ'),
                            'top_headlines': [article['title'] for article in news_data.get('top_articles', [])[:5]]
                        },
                        
                        # Risk management
                        'risk_management': {
                            'stop_loss': signal_data.get('stop_loss', 0),
                            'take_profit': signal_data.get('take_profit', 0),
                            'risk_reward_ratio': signal_data.get('risk_reward_ratio', 0),
                            'position_size_suggestion': signal_data.get('position_size_suggestion', 10),
                            'max_risk_per_trade': signal_data.get('max_risk_per_trade', 2)
                        },
                        
                        # Performance metrics
                        'performance': {
                            'analysis_reliability': min(1.0, len(technical_data) / 200) if technical_data is not None else 0,
                            'data_quality_score': self._calculate_data_quality_score(technical_data, news_data),
                            'component_scores': signal_data.get('technical_components', {}),
                            'weights_used': signal_data.get('technical_weights', {})
                        }
                    }
                    
                    # Save comprehensive report
                    with open('gold_analysis_enhanced.json', 'w', encoding='utf-8') as f:
                        json.dump(final_report, f, ensure_ascii=False, indent=2, default=str)
                    
                    logger.info("‚úÖ ÿ™ŸÖ ÿ•ŸÜÿ¨ÿßÿ≤ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ¥ÿßŸÖŸÑ ÿ®ŸÜÿ¨ÿßÿ≠")
                    
                    # Print enhanced summary
                    self._print_enhanced_summary(signal_data, news_data, execution_time, analysis_id)
                    
                    return final_report
                    
                except Exception as e:
                    logger.error(f"‚ùå ŸÅÿ¥ŸÑ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ¥ÿßŸÖŸÑ: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    return {
                        'status': 'error',
                        'error': str(e),
                        'timestamp': datetime.now().isoformat(),
                        'execution_time_ms': int((time.time() - start_time) * 1000)
                    }
        
            def _calculate_data_quality_score(self, technical_data, news_data) -> float:
                """ÿ≠ÿ≥ÿßÿ® ŸÜŸÇÿßÿ∑ ÿ¨ŸàÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™"""
                quality_score = 0
                
                # Technical data quality
                if technical_data is not None and not technical_data.empty:
                    data_completeness = len(technical_data) / 500  # Target 500 days
                    quality_score += min(0.5, data_completeness)
                
                # News data quality
                if news_data.get('status') == 'success':
                    news_completeness = news_data.get('articles_count', 0) / 30  # Target 30 articles
                    quality_score += min(0.5, news_completeness)
                
                return round(min(1.0, quality_score), 3)
        
            def _print_enhanced_summary(self, signal_data, news_data, execution_time, analysis_id):
                """ÿ∑ÿ®ÿßÿπÿ© ÿßŸÑŸÖŸÑÿÆÿµ ÿßŸÑŸÖÿ≠ÿ≥ŸÜ"""
                logger.info("")
                logger.info("=" * 60)
                logger.info("üìã ŸÖŸÑÿÆÿµ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÜŸáÿßÿ¶Ÿä")
                logger.info("=" * 60)
                logger.info(f"üéØ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ©: {signal_data.get('signal', 'N/A')} ({signal_data.get('strength', 'N/A')})")
                logger.info(f"üìä ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸäÿ©: {signal_data.get('total_score', 0):.4f}")
                logger.info(f"üîí ŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑÿ´ŸÇÿ©: {signal_data.get('confidence', 0):.1%}")
                logger.info(f"üí∞ ÿ≥ÿπÿ± ÿßŸÑÿ∞Ÿáÿ®: ${signal_data.get('current_price', 0):,.2f}")
                logger.info(f"üõë ŸàŸÇŸÅ ÿßŸÑÿÆÿ≥ÿßÿ±ÿ©: ${signal_data.get('stop_loss', 0):,.2f}")
                logger.info(f"üéØ ÿßŸÑŸáÿØŸÅ: ${signal_data.get('take_profit', 0):,.2f}")
                logger.info(f"üìà ŸÇŸàÿ© ÿßŸÑÿßÿ™ÿ¨ÿßŸá: {signal_data.get('trend_strength', 'N/A')}")
                logger.info(f"‚ö° ÿ≠ÿßŸÑÿ© ÿßŸÑÿ≤ÿÆŸÖ: {signal_data.get('momentum_status', 'N/A')}")
                logger.info(f"üìä ÿ≠ÿßŸÑÿ© ÿßŸÑÿ≥ŸàŸÇ: {signal_data.get('market_condition', 'normal')}")
                logger.info(f"üì∞ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ£ÿÆÿ®ÿßÿ±: {news_data.get('status', 'unknown')} ({news_data.get('articles_count', 0)} ŸÖŸÇÿßŸÑ)")
                logger.info(f"üí≠ ŸÖÿ¥ÿßÿπÿ± ÿßŸÑÿ£ÿÆÿ®ÿßÿ±: {news_data.get('sentiment_score', 0):+.4f}")
                logger.info(f"‚è±Ô∏è ŸàŸÇÿ™ ÿßŸÑÿ™ŸÜŸÅŸäÿ∞: {execution_time}ms")
                logger.info(f"üÜî ŸÖÿπÿ±ŸÅ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ: {analysis_id}")
        
        def main():
            """ÿßŸÑÿØÿßŸÑÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ÿßŸÑŸÖÿ≠ÿ≥ŸÜÿ©"""
            try:
                print("üèÜ ŸÖÿ≠ŸÑŸÑ ÿßŸÑÿ∞Ÿáÿ® ÿßŸÑŸÖÿ≠ÿ≥ŸÜ - ŸÜÿ≥ÿÆÿ© ÿÆŸÅŸäŸÅÿ© ŸàŸÇŸàŸäÿ©")
                print("=" * 60)
                
                analyzer = EnhancedGoldAnalyzer()
                result = analyzer.run_enhanced_comprehensive_analysis()
                
                if result.get('status') == 'success':
                    print("\nüéâ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÉÿßŸÖŸÑ ŸÅŸä:")
                    print(" - gold_analysis_enhanced.json")
                    print(" - gold_analysis_history.db") 
                    print(" - gold_analysis.log")
                    
                    # Display top headlines
                    news_headlines = result.get('news_analysis', {}).get('top_headlines', [])
                    if news_headlines:
                        print(f"\nüì∞ ÿ£ŸáŸÖ {len(news_headlines)} ÿ£ÿÆÿ®ÿßÿ± ŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿßŸÑÿ∞Ÿáÿ®:")
                        for i, headline in enumerate(news_headlines, 1):
                            print(f" {i}. {headline[:80]}..." + (" [" + headline.split(']')[-1] + "]" if ']' in headline else ""))
                    
                    return 0
                else:
                    print(f"‚ùå ŸÅÿ¥ŸÑ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ: {result.get('error', 'ÿÆÿ∑ÿ£ ÿ∫Ÿäÿ± ŸÖÿπÿ±ŸàŸÅ')}")
                    return 1
                    
            except Exception as e:
                print(f"üí• ÿÆÿ∑ÿ£ ÿ≠ÿ±ÿ¨: {e}")
                return 1
        
        if __name__ == "__main__":
            exit(main())
        EOF

    - name: üöÄ Execute Enhanced Analysis
      id: analysis
      run: |
        echo "üöÄ Starting Enhanced Gold Analysis (Lightweight)..."
        timeout 720 python enhanced_analyzer.py
        echo "analysis_status=success" >> $GITHUB_OUTPUT

    - name: üìä Display Enhanced Results
      if: steps.analysis.outputs.analysis_status == 'success'
      run: |
        if [ -f "gold_analysis_enhanced.json" ]; then
          echo "üéâ Enhanced Analysis Complete!"
          echo "================================"
          
          python -c "
        import json
        
        with open('gold_analysis_enhanced.json', 'r') as f:
            data = json.load(f)
        
        signal = data['signal']
        technical = data['technical_analysis']
        news = data['news_analysis']
        risk = data['risk_management']
        
        print(f'üéØ Signal: {signal[\"primary_signal\"]} ({signal[\"strength\"]})')
        print(f'üìä Total Score: {signal[\"total_score\"]:.4f}')
        print(f'üîí Confidence: {signal[\"confidence\"]:.1%}')
        print(f'üí∞ Gold Price: \${technical[\"current_price\"]:,.2f}')
        print(f'üõë Stop Loss: \${risk[\"stop_loss\"]:,.2f}')
        print(f'üéØ Take Profit: \${risk[\"take_profit\"]:,.2f}')
        print(f'üìà RSI: {technical[\"rsi\"]}')
        print(f'üìä MACD: {technical[\"macd_signal\"]}')
        print(f'üìà Trend: {technical[\"trend_strength\"]}')
        print(f'üì∞ News Articles: {news[\"articles_count\"]}')
        print(f'üí≠ News Sentiment: {news[\"sentiment_score\"]:+.4f}')
        print(f'‚ö° Execution: {data[\"execution_time_ms\"]}ms')
        print(f'üÜî Analysis ID: {data[\"analysis_id\"]}')
        "
        else
          echo "‚ùå Results file not found"
        fi

    - name: üìù Create Enhanced README
      if: steps.analysis.outputs.analysis_status == 'success'
      run: |
        python -c "
        import json
        from datetime import datetime
        
        with open('gold_analysis_enhanced.json', 'r') as f:
            data = json.load(f)
        
        signal = data['signal']
        technical = data['technical_analysis']
        news = data['news_analysis']
        risk = data['risk_management']
        
        readme = f'''# üèÜ Enhanced Gold Analysis System
        
        **Generated:** {datetime.fromisoformat(data['timestamp']).strftime('%Y-%m-%d %H:%M:%S UTC')}
        
        ## üéØ Trading Signal
        
        **{signal['primary_signal']}** - {signal['strength']}
        
        - **Total Score:** {signal['total_score']:.4f}
        - **Confidence:** {signal['confidence']:.1%}
        - **Technical Score:** {signal['technical_score']:+.4f} (70%)
        - **News Score:** {signal['news_score']:+.4f} (30%)
        
        ## üí∞ Market Analysis
        
        | Metric | Value |
        |--------|-------|
        | Gold Price | \${technical['current_price']:,.2f} |
        | RSI (14) | {technical['rsi']:.1f} |
        | MACD Signal | {technical['macd_signal']} |
        | Bollinger Position | {technical['bb_position']:.1%} |
        | Volume Trend | {technical['volume_trend']} |
        | Volatility | {technical['volatility']:.1%} |
        | Market Condition | {technical['market_condition'].replace('_', ' ').title()} |
        | Trend Strength | {technical['trend_strength']} |
        | Momentum Status | {technical['momentum_status']} |
        
        ## üîß Risk Management
        
        - **Stop Loss:** \${risk['stop_loss']:,.2f}
        - **Take Profit:** \${risk['take_profit']:,.2f}
        - **Risk/Reward Ratio:** {risk['risk_reward_ratio']:.2f}
        - **Suggested Position Size:** {risk['position_size_suggestion']:.1f}%
        - **Max Risk Per Trade:** {risk['max_risk_per_trade']:.1f}%
        
        ## üì∞ News Analysis
        
        - **Status:** {news['status'].title()}
        - **Articles Analyzed:** {news['articles_count']}
        - **Sentiment Score:** {news['sentiment_score']:+.4f}
        - **Positive/Negative/Neutral:** {news['positive_count']}/{news['negative_count']}/{news['neutral_count']}
        - **Analysis Confidence:** {news['confidence']:.1%}
        '''
        
        headlines = news.get('top_headlines', [])
        if headlines:
            readme += '''
        ## üìã Top Headlines
        '''
            for i, headline in enumerate(headlines, 1):
                readme += f'''
        {i}. {headline}
        '''
        
        readme += f'''
        
        ## üìä Performance Metrics
        
        - **Analysis ID:** {data['analysis_id']}
        - **Execution Time:** {data['execution_time_ms']}ms
        - **Data Points:** {technical['data_points']}
        - **Data Quality Score:** {data['performance']['data_quality_score']:.2f}
        - **Analysis Version:** {data['analysis_version']}
        
        ---
        
        *Enhanced lightweight analysis with comprehensive sentiment analysis and advanced technical indicators*
        '''
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
        
        print('‚úÖ Enhanced README created successfully')
        "

    - name: üíæ Commit Enhanced Results
      if: steps.analysis.outputs.analysis_status == 'success'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "üèÜ Enhanced Gold Analysis Bot"
        
        git add gold_analysis_enhanced.json README.md
        git add gold_analysis_history.db gold_analysis.log
        
        if git diff --staged --quiet; then
          echo "‚ö†Ô∏è No changes to commit"
        else
          SIGNAL=$(python -c "import json; data=json.load(open('gold_analysis_enhanced.json')); print(data['signal']['primary_signal'])" 2>/dev/null || echo 'Enhanced Analysis')
          SCORE=$(python -c "import json; data=json.load(open('gold_analysis_enhanced.json')); print(f\"{data['signal']['total_score']:.3f}\")" 2>/dev/null || echo '0.000')
          
          git commit -m "üèÜ Enhanced Gold Analysis: $SIGNAL ($SCORE) - $(date -u '+%Y-%m-%d %H:%M UTC')"
          git push
          echo "‚úÖ Enhanced results committed and pushed"
        fi

    - name: üì§ Upload Enhanced Analysis
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: enhanced-gold-analysis
        path: |
          gold_analysis_enhanced.json
          gold_analysis_history.db
          gold_analysis.log
          README.md
        retention-days: 90

    - name: üéâ Enhanced Success Summary
      if: success()
      run: |
        echo "üèÜ ENHANCED GOLD ANALYSIS COMPLETE!"
        echo "===================================="
        
        python -c "
        try:
            import json
            with open('gold_analysis_enhanced.json', 'r') as f:
                data = json.load(f)
            
            signal = data['signal']
            news = data['news_analysis']
            
            print(f'üéØ Final Signal: {signal[\"primary_signal\"]} ({signal[\"strength\"]})')
            print(f'üìä Total Score: {signal[\"total_score\"]:.4f}')
            print(f'üîí Confidence: {signal[\"confidence\"]:.1%}')
            print(f'üí∞ Gold Price: \${data[\"technical_analysis\"][\"current_price\"]:,.2f}')
            print(f'üì∞ News Articles: {news[\"articles_count\"]} analyzed')
            print(f'üí≠ News Sentiment: {news[\"sentiment_score\"]:+.4f}')
            print(f'‚ö° Execution: {data[\"execution_time_ms\"]}ms')
            print(f'üÜî Analysis ID: {data[\"analysis_id\"]}')
            print(f'\\nüèÜ Enhanced lightweight analysis with professional-grade results!')
        except Exception as e:
            print(f'‚úÖ Analysis completed successfully (summary error: {e})')
        "