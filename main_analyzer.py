#!/usr/bin/env python3
"""
ğŸ† Ù…Ø­Ù„Ù„ Ø§Ù„Ø°Ù‡Ø¨ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±
Professional Gold Analyzer with Advanced Features

Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ù…Ù†Ø©:
âœ… ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ (asyncio, threading, caching)
âœ… ØªØ­Ù„ÙŠÙ„ Ø£Ø®Ø¨Ø§Ø± Ù…Ø­Ø³Ù† ÙˆÙ…ØªØ®ØµØµ
âœ… Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ© Ù…ØªØ®ØµØµØ© Ø¨Ø§Ù„Ø°Ù‡Ø¨
âœ… Ù†Ø¸Ø§Ù… Backtesting Ø´Ø§Ù…Ù„
"""

import asyncio
import aiohttp
import yfinance as yf
import pandas as pd
import numpy as np
import requests
import json
import os
import sqlite3
import logging
import warnings
from datetime import datetime, timedelta
from transformers import pipeline
import pandas_ta as ta
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import time
from typing import Dict, List, Tuple, Optional, Union
import threading
from dataclasses import dataclass
import pickle
from pathlib import Path

warnings.filterwarnings('ignore')

# =============================================================================
# ğŸ“‹ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù†
# =============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gold_analysis_advanced.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”§ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø³Ø±Ø¹Ø©
# =============================================================================

@dataclass
class PerformanceConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    max_workers: int = 4
    request_timeout: int = 15
    cache_duration: int = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
    max_articles_per_query: int = 30
    sentiment_batch_size: int = 5

class AsyncDataFetcher:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù† Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    
    def __init__(self, timeout: int = 15):
        self.timeout = timeout
        self.session = None
    
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(limit=10, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_news_async(self, url: str) -> Optional[Dict]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.json()
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {e}")
        return None

class DataCache:
    """Ù†Ø¸Ø§Ù… ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self._lock = threading.Lock()
    
    def _get_cache_path(self, key: str) -> Path:
        return self.cache_dir / f"{key}.pkl"
    
    def get(self, key: str, max_age_seconds: int = 300) -> Optional[any]:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        cache_path = self._get_cache_path(key)
        
        if not cache_path.exists():
            return None
        
        # ÙØ­Øµ Ø¹Ù…Ø± Ø§Ù„Ù…Ù„Ù
        if (time.time() - cache_path.stat().st_mtime) > max_age_seconds:
            return None
        
        try:
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        except Exception:
            return None
    
    def set(self, key: str, value: any):
        """Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        with self._lock:
            try:
                cache_path = self._get_cache_path(key)
                with open(cache_path, 'wb') as f:
                    pickle.dump(value, f)
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")

# =============================================================================
# ğŸ“° ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø­Ø³Ù† ÙˆØ§Ù„Ù…ØªØ®ØµØµ
# =============================================================================

class EnhancedNewsAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø£Ø®Ø¨Ø§Ø± Ù…ØªØ·ÙˆØ± ÙˆÙ…ØªØ®ØµØµ Ù„Ù„Ø°Ù‡Ø¨"""
    
    def __init__(self, api_key: str, sentiment_pipeline):
        self.api_key = api_key
        self.sentiment_pipeline = sentiment_pipeline
        self.cache = DataCache("news_cache")
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…ØªØ®ØµØµØ© Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ù…Ø­Ø³Ù†Ø©
        self.gold_keywords = {
            # Ø§Ù„Ø°Ù‡Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± - ÙˆØ²Ù† Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹
            'gold': 8, 'xau/usd': 8, 'xauusd': 8, 'bullion': 7, 'precious metal': 7,
            'gold price': 8, 'gold futures': 7, 'gold etf': 6, 'gld': 5,
            
            # Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ù†Ù‚Ø¯ÙŠØ© - ÙˆØ²Ù† Ø¹Ø§Ù„ÙŠ
            'federal reserve': 6, 'fed': 6, 'jerome powell': 6, 'fomc': 6,
            'interest rate': 6, 'rate cut': 7, 'rate hike': 7, 'monetary policy': 6,
            'quantitative easing': 6, 'tapering': 5, 'dovish': 5, 'hawkish': 5,
            
            # Ø§Ù„ØªØ¶Ø®Ù… ÙˆØ§Ù„Ø§Ù‚ØªØµØ§Ø¯ - ÙˆØ²Ù† Ù…ØªÙˆØ³Ø· Ø¹Ø§Ù„ÙŠ
            'inflation': 6, 'cpi': 6, 'pce': 5, 'consumer price': 6,
            'core inflation': 6, 'deflation': 5, 'stagflation': 6,
            'economic data': 4, 'gdp': 4, 'unemployment': 4, 'nfp': 5,
            
            # Ø§Ù„Ø¯ÙˆÙ„Ø§Ø± ÙˆØ§Ù„Ø¹Ù…Ù„Ø§Øª - ÙˆØ²Ù† Ù…ØªÙˆØ³Ø·
            'dollar': 4, 'dxy': 5, 'dollar index': 5, 'usd': 3,
            'dollar strength': 5, 'dollar weakness': 6, 'currency': 3,
            
            # Ø§Ù„Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ© ÙˆØ§Ù„Ø£Ø²Ù…Ø§Øª - ÙˆØ²Ù† Ù…ØªÙˆØ³Ø· Ø¹Ø§Ù„ÙŠ  
            'geopolitical': 5, 'safe haven': 7, 'safe-haven': 7, 'risk-off': 6,
            'war': 5, 'conflict': 5, 'tension': 4, 'sanctions': 4,
            'trade war': 4, 'tariff': 4, 'crisis': 5, 'recession': 6,
            
            # Ø£Ø³ÙˆØ§Ù‚ Ø£Ø®Ø±Ù‰ Ù…Ø¤Ø«Ø±Ø© - ÙˆØ²Ù† Ù…Ù†Ø®ÙØ¶
            'stock market': 2, 'bonds': 3, 'treasury': 3, 'yield': 3,
            'oil': 2, 'commodities': 3, 'mining': 3, 'central bank': 4
        }
        
        # Ù…ØµØ§Ø¯Ø± Ø£Ø®Ø¨Ø§Ø± Ù…ÙˆØ«ÙˆÙ‚Ø© Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†
        self.trusted_sources = {
            'Reuters': 1.2, 'Bloomberg': 1.2, 'MarketWatch': 1.1,
            'CNBC': 1.1, 'Financial Times': 1.2, 'Wall Street Journal': 1.2,
            'Yahoo Finance': 1.0, 'Investing.com': 1.0, 'Kitco': 1.3,
            'GoldSeek': 1.3, 'BullionVault': 1.2
        }
    
    async def fetch_multi_source_news(self) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ"""
        news_queries = [
            'gold OR XAU OR bullion OR "precious metals"',
            '"interest rates" OR "federal reserve" OR "jerome powell" OR FOMC',
            'inflation OR CPI OR "consumer prices" OR "monetary policy"',
            '"dollar index" OR DXY OR "dollar strength" OR USD',
            'geopolitical OR "safe haven" OR "risk off" OR crisis',
            '"gold price" OR "gold futures" OR "gold mining" OR GLD'
        ]
        
        all_articles = []
        
        async with AsyncDataFetcher(timeout=20) as fetcher:
            tasks = []
            
            for query in news_queries:
                url = (
                    f"https://newsapi.org/v2/everything?"
                    f"q={query}&language=en&sortBy=publishedAt&"
                    f"pageSize=25&"
                    f"from={(datetime.now() - timedelta(days=2)).date()}&"
                    f"apiKey={self.api_key}"
                )
                tasks.append(fetcher.fetch_news_async(url))
            
            # ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, dict) and 'articles' in result:
                    all_articles.extend(result['articles'])
        
        logger.info(f"ğŸ” ØªÙ… Ø¬Ù„Ø¨ {len(all_articles)} Ù…Ù‚Ø§Ù„Ø§Ù‹ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±")
        return all_articles
    
    def calculate_relevance_score(self, article: Dict) -> Tuple[float, List[str]]:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ø°Ù‡Ø¨ Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        title = (article.get('title', '') or '').lower()
        description = (article.get('description', '') or '').lower()
        content = f"{title} {description}"
        
        score = 0
        matched_keywords = []
        
        for keyword, weight in self.gold_keywords.items():
            if keyword in content:
                score += weight
                matched_keywords.append(keyword)
        
        # Ù…ÙƒØ§ÙØ£Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©
        source_name = article.get('source', {}).get('name', '')
        if source_name in self.trusted_sources:
            score *= self.trusted_sources[source_name]
        
        return score, matched_keywords[:5]  # Ø£Ù‡Ù… 5 ÙƒÙ„Ù…Ø§Øª
    
    def batch_sentiment_analysis(self, texts: List[str]) -> List[Dict]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ø§Ù„Ø¯ÙØ¹Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        results = []
        batch_size = PerformanceConfig.sentiment_batch_size
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø©
                batch_results = []
                for text in batch:
                    if len(text.strip()) < 10:
                        batch_results.append({'positive': 0, 'negative': 0, 'neutral': 1})
                        continue
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
                    sentiment_output = self.sentiment_pipeline(text[:300])  # Ù‚Ø·Ø¹ Ø§Ù„Ù†Øµ
                    
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ù„Ù‰ format Ù…ÙˆØ­Ø¯
                    sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}
                    for item in sentiment_output[0]:
                        sentiment_scores[item['label']] = item['score']
                    
                    batch_results.append(sentiment_scores)
                
                results.extend(batch_results)
                
            except Exception as e:
                logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¯ÙØ¹Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {e}")
                # Ø¥Ø¶Ø§ÙØ© Ù†ØªØ§Ø¦Ø¬ Ù…Ø­Ø§ÙŠØ¯Ø© Ù„Ù„Ø¯ÙØ¹Ø© Ø§Ù„ÙØ§Ø´Ù„Ø©
                results.extend([{'positive': 0, 'negative': 0, 'neutral': 1}] * len(batch))
        
        return results
    
    async def run_enhanced_analysis(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ø£Ø®Ø¨Ø§Ø±"""
        logger.info("ğŸ“° Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØªØ·ÙˆØ±...")
        
        if not self.api_key or not self.sentiment_pipeline:
            return self._get_default_result("missing_requirements")
        
        try:
            # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            cache_key = f"news_analysis_{datetime.now().strftime('%Y%m%d_%H')}"
            cached_result = self.cache.get(cache_key, max_age_seconds=1800)  # 30 Ø¯Ù‚ÙŠÙ‚Ø©
            if cached_result:
                logger.info("ğŸ“¦ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ØªØ§Ø¦Ø¬ Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹")
                return cached_result
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
            all_articles = await self.fetch_multi_source_news()
            
            if not all_articles:
                return self._get_default_result("no_articles")
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø± ÙˆÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            unique_articles = self._remove_duplicates(all_articles)
            logger.info(f"ğŸ¯ ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ {len(unique_articles)} Ù…Ù‚Ø§Ù„Ø§Ù‹ ÙØ±ÙŠØ¯Ø§Ù‹")
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ø°Ù‡Ø¨
            relevant_articles = []
            for article in unique_articles:
                relevance_score, matched_keywords = self.calculate_relevance_score(article)
                
                # Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
                if relevance_score >= 4:  # Ø­Ø¯ Ù…Ù‚Ø¨ÙˆÙ„
                    article['relevance_score'] = relevance_score
                    article['matched_keywords'] = matched_keywords
                    relevant_articles.append(article)
            
            if not relevant_articles:
                return self._get_default_result("no_relevant")
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø© ÙˆØ§Ù„Ø­Ø¯Ø§Ø«Ø©
            relevant_articles.sort(key=lambda x: (x['relevance_score'], x.get('publishedAt', '')), reverse=True)
            top_articles = relevant_articles[:40]  # Ø£ÙØ¶Ù„ 40 Ù…Ù‚Ø§Ù„
            
            logger.info(f"ğŸ”¥ ØªÙ… Ø§Ø®ØªÙŠØ§Ø± {len(top_articles)} Ù…Ù‚Ø§Ù„Ø§Ù‹ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØµÙ„Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            texts_for_analysis = []
            for article in top_articles:
                text = f"{article.get('title', '')} {article.get('description', '')}"
                texts_for_analysis.append(text)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ø§Ù„Ø¯ÙØ¹Ø§Øª
            sentiment_results = self.batch_sentiment_analysis(texts_for_analysis)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            weighted_sentiments = []
            processed_articles = []
            
            for i, (article, sentiment) in enumerate(zip(top_articles, sentiment_results)):
                try:
                    # Ø­Ø³Ø§Ø¨ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
                    sentiment_score = sentiment['positive'] - sentiment['negative']
                    
                    # ÙˆØ²Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø­Ø³Ø¨ ØµÙ„Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø§Ù„Ø°Ù‡Ø¨
                    relevance_weight = min(article['relevance_score'] / 10, 1.5)
                    weighted_sentiment = sentiment_score * relevance_weight
                    
                    weighted_sentiments.append(weighted_sentiment)
                    
                    # Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
                    processed_articles.append({
                        'title': article['title'],
                        'source': article.get('source', {}).get('name', 'Unknown'),
                        'sentiment_score': round(sentiment_score, 3),
                        'weighted_sentiment': round(weighted_sentiment, 3),
                        'relevance_score': round(article['relevance_score'], 1),
                        'matched_keywords': article['matched_keywords'],
                        'published_at': article.get('publishedAt', ''),
                        'confidence': round(max(sentiment.values()), 3)
                    })
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‚Ø§Ù„: {e}")
                    continue
            
            if not weighted_sentiments:
                return self._get_default_result("analysis_failed")
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            final_sentiment = np.mean(weighted_sentiments)
            sentiment_volatility = np.std(weighted_sentiments)
            confidence_level = 1 - (sentiment_volatility / (abs(final_sentiment) + 0.1))
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ù„Ù„Ø¹Ø±Ø¶
            processed_articles.sort(key=lambda x: (x['relevance_score'], abs(x['weighted_sentiment'])), reverse=True)
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            result = {
                "status": "success",
                "news_score": round(final_sentiment, 3),
                "confidence_level": round(max(0, min(1, confidence_level)), 3),
                "headlines": processed_articles[:10],  # Ø£Ù‡Ù… 10 Ù…Ù‚Ø§Ù„Ø§Øª
                "analysis_details": {
                    'total_articles_analyzed': len(processed_articles),
                    'average_sentiment': round(final_sentiment, 3),
                    'sentiment_volatility': round(sentiment_volatility, 3),
                    'positive_articles': len([a for a in processed_articles if a['weighted_sentiment'] > 0.1]),
                    'negative_articles': len([a for a in processed_articles if a['weighted_sentiment'] < -0.1]),
                    'neutral_articles': len([a for a in processed_articles if abs(a['weighted_sentiment']) <= 0.1]),
                    'high_relevance_articles': len([a for a in processed_articles if a['relevance_score'] > 8]),
                    'confidence_distribution': {
                        'high_confidence': len([a for a in processed_articles if a['confidence'] > 0.8]),
                        'medium_confidence': len([a for a in processed_articles if 0.6 < a['confidence'] <= 0.8]),
                        'low_confidence': len([a for a in processed_articles if a['confidence'] <= 0.6])
                    }
                }
            }
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            self.cache.set(cache_key, result)
            
            logger.info(f"ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…ÙƒØªÙ…Ù„: Ø§Ù„Ù†ØªÙŠØ¬Ø© {final_sentiment:.3f} (Ø«Ù‚Ø©: {confidence_level:.3f})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {e}")
            return self._get_default_result("error", str(e))
    
    def _remove_duplicates(self, articles: List[Dict]) -> List[Dict]:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            title = (article.get('title', '') or '').lower().strip()
            if title and title not in seen_titles and len(title) > 10:
                seen_titles.add(title)
                unique_articles.append(article)
        
        return unique_articles
    
    def _get_default_result(self, status: str, error_msg: str = "") -> Dict:
        """Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        return {
            "status": status,
            "news_score": 0,
            "confidence_level": 0,
            "headlines": [],
            "analysis_details": {"error": error_msg} if error_msg else {}
        }

# =============================================================================
# ğŸ“ˆ Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ© Ù…ØªØ®ØµØµØ© Ø¨Ø§Ù„Ø°Ù‡Ø¨
# =============================================================================

class GoldSpecificIndicators:
    """Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ© Ù…ØªØ®ØµØµØ© Ø¨Ø§Ù„Ø°Ù‡Ø¨"""
    
    @staticmethod
    def gold_silver_ratio(gold_prices: pd.Series, silver_prices: pd.Series) -> pd.Series:
        """Ù†Ø³Ø¨Ø© Ø§Ù„Ø°Ù‡Ø¨ Ø¥Ù„Ù‰ Ø§Ù„ÙØ¶Ø© - Ù…Ø¤Ø´Ø± Ù…Ù‡Ù… Ù„Ù„Ù…Ø¹Ø§Ø¯Ù† Ø§Ù„Ø«Ù…ÙŠÙ†Ø©"""
        return gold_prices / silver_prices
    
    @staticmethod
    def gold_oil_ratio(gold_prices: pd.Series, oil_prices: pd.Series) -> pd.Series:
        """Ù†Ø³Ø¨Ø© Ø§Ù„Ø°Ù‡Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù†ÙØ· - Ù…Ø¤Ø´Ø± Ù„Ù„ØªØ¶Ø®Ù…"""
        return gold_prices / oil_prices
    
    @staticmethod
    def seasonal_strength(prices: pd.Series) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ© Ù„Ù„Ø°Ù‡Ø¨"""
        df = pd.DataFrame({'price': prices, 'date': prices.index})
        df['month'] = df['date'].dt.month
        df['returns'] = df['price'].pct_change()
        
        monthly_performance = df.groupby('month')['returns'].agg(['mean', 'std', 'count'])
        
        # Ø£Ø´Ù‡Ø± Ù‚ÙˆØ© Ø§Ù„Ø°Ù‡Ø¨ ØªØ§Ø±ÙŠØ®ÙŠØ§Ù‹
        strong_months = [1, 2, 8, 9, 12]  # ÙŠÙ†Ø§ÙŠØ±ØŒ ÙØ¨Ø±Ø§ÙŠØ±ØŒ Ø£ØºØ³Ø·Ø³ØŒ Ø³Ø¨ØªÙ…Ø¨Ø±ØŒ Ø¯ÙŠØ³Ù…Ø¨Ø±
        current_month = datetime.now().month
        
        seasonal_score = 1 if current_month in strong_months else -1
        
        return {
            'seasonal_score': seasonal_score,
            'current_month_historical_return': monthly_performance.loc[current_month, 'mean'] if current_month in monthly_performance.index else 0,
            'is_strong_season': current_month in strong_months,
            'monthly_stats': monthly_performance.to_dict('index')
        }
    
    @staticmethod
    def support_resistance_levels(prices: pd.Series, window: int = 20) -> Dict:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„Ø°Ù‡Ø¨"""
        high_prices = prices.rolling(window=window, center=True).max()
        low_prices = prices.rolling(window=window, center=True).min()
        
        current_price = prices.iloc[-1]
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© (Ø§Ù„Ù‚Ù…Ù… Ø§Ù„Ù…Ø­Ù„ÙŠØ©)
        resistance_levels = []
        support_levels = []
        
        for i in range(window, len(prices) - window):
            if prices.iloc[i] == high_prices.iloc[i]:
                resistance_levels.append(prices.iloc[i])
            if prices.iloc[i] == low_prices.iloc[i]:
                support_levels.append(prices.iloc[i])
        
        # Ø£Ù‚Ø±Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø¯Ø¹Ù… ÙˆÙ…Ù‚Ø§ÙˆÙ…Ø©
        resistance_above = [r for r in resistance_levels if r > current_price]
        support_below = [s for s in support_levels if s < current_price]
        
        nearest_resistance = min(resistance_above) if resistance_above else None
        nearest_support = max(support_below) if support_below else None
        
        return {
            'current_price': current_price,
            'nearest_resistance': nearest_resistance,
            'nearest_support': nearest_support,
            'resistance_distance': (nearest_resistance - current_price) / current_price * 100 if nearest_resistance else None,
            'support_distance': (current_price - nearest_support) / current_price * 100 if nearest_support else None,
            'total_resistance_levels': len(resistance_levels),
            'total_support_levels': len(support_levels)
        }
    
    @staticmethod
    def gold_volatility_regime(prices: pd.Series, short_window: int = 10, long_window: int = 30) -> Dict:
        """ØªØ­Ø¯ÙŠØ¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ø°Ù‡Ø¨"""
        returns = prices.pct_change().dropna()
        
        short_vol = returns.rolling(short_window).std() * np.sqrt(252)  # Ø³Ù†ÙˆÙŠ
        long_vol = returns.rolling(long_window).std() * np.sqrt(252)
        
        current_short_vol = short_vol.iloc[-1]
        current_long_vol = long_vol.iloc[-1]
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
        if current_short_vol > 0.25:
            regime = "high_volatility"
            regime_score = 1  # Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ø¹Ø§Ù„ÙŠØ© ØªÙÙŠØ¯ Ø§Ù„Ø°Ù‡Ø¨
        elif current_short_vol < 0.15:
            regime = "low_volatility" 
            regime_score = -0.5
        else:
            regime = "normal_volatility"
            regime_score = 0
        
        # Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
        vol_trend = "increasing" if current_short_vol > current_long_vol else "decreasing"
        
        return {
            'current_volatility': round(current_short_vol, 3),
            'average_volatility': round(current_long_vol, 3),
            'volatility_regime': regime,
            'regime_score': regime_score,
            'volatility_trend': vol_trend,
            'volatility_percentile': round(
                (returns.rolling(252).std().iloc[-1] > returns.rolling(252).std().quantile(0.75)) * 100, 1
            )
        }
    
    @staticmethod
    def cot_simulation(prices: pd.Series) -> Dict:
        """Ù…Ø­Ø§ÙƒØ§Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ²Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø± (COT) Ù„Ù„Ø°Ù‡Ø¨"""
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· ÙŠØ­Ø§ÙƒÙŠ Ø³Ù„ÙˆÙƒ COT
        returns = prices.pct_change().dropna()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ (ØªØ¬Ø§Ø± ØªØ¬Ø§Ø±ÙŠÙˆÙ†)
        long_term_trend = prices.rolling(50).mean().pct_change().iloc[-1]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¶Ø§Ø±Ø¨Ø© (ØµÙ†Ø§Ø¯ÙŠÙ‚ Ø§Ù„ØªØ­ÙˆØ·)
        short_term_momentum = returns.rolling(10).mean().iloc[-1]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ¯Ø§ÙˆÙ„ÙŠÙ† Ø§Ù„ØµØºØ§Ø±
        price_vs_ma = (prices.iloc[-1] - prices.rolling(20).mean().iloc[-1]) / prices.iloc[-1] * 100
        
        # ØªÙ‚Ø¯ÙŠØ± Ù…Ø±Ø§ÙƒØ² COT
        commercial_position = -long_term_trend * 10  # Ø§Ù„ØªØ¬Ø§Ø±ÙŠÙˆÙ† Ø¹Ø§Ø¯Ø© Ø¹ÙƒØ³ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        hedge_fund_position = short_term_momentum * 10  # ØµÙ†Ø§Ø¯ÙŠÙ‚ Ø§Ù„ØªØ­ÙˆØ· Ù…Ø¹ Ø§Ù„Ø²Ø®Ù…
        retail_position = price_vs_ma / 10  # Ø§Ù„ØµØºØ§Ø± ÙŠØªØ¨Ø¹ÙˆÙ† Ø§Ù„Ø³Ø¹Ø±
        
        return {
            'commercial_net_position': round(commercial_position, 2),
            'hedge_fund_net_position': round(hedge_fund_position, 2),
            'retail_net_position': round(retail_position, 2),
            'market_sentiment': 'bullish' if hedge_fund_position > 0 else 'bearish',
            'commercial_signal': 'buy' if commercial_position < -0.5 else 'sell' if commercial_position > 0.5 else 'neutral'
        }

# =============================================================================
# ğŸ”¬ Ù†Ø¸Ø§Ù… Backtesting Ø§Ù„Ø´Ø§Ù…Ù„
# =============================================================================

class GoldBacktestEngine:
    """Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± ØªØ§Ø±ÙŠØ®ÙŠ Ø´Ø§Ù…Ù„ Ù„Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©"""
    
    def __init__(self, initial_capital: float = 10000):
        self.initial_capital = initial_capital
        self.commission = 0.001  # 0.1% Ø¹Ù…ÙˆÙ„Ø©
        
    def prepare_backtest_data(self, market_data: pd.DataFrame, signals_data: pd.DataFrame) -> pd.DataFrame:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ"""
        # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        backtest_df = market_data.copy()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª (Ù…Ø­Ø§ÙƒØ§Ø©)
        backtest_df['signal'] = self._simulate_historical_signals(backtest_df)
        backtest_df['position'] = backtest_df['signal'].shift(1)  # ØªØ£Ø®ÙŠØ± ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯ Ù„Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©
        
        return backtest_df.dropna()
    
    def _simulate_historical_signals(self, data: pd.DataFrame) -> pd.Series:
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©"""
        signals = []
        
        for i in range(len(data)):
            if i < 200:  # Ù†Ø­ØªØ§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ù…Ø¤Ø´Ø±Ø§Øª
                signals.append(0)
                continue
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ø¨Ø³Ø·Ø© Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©
            current_slice = data.iloc[max(0, i-200):i+1]
            price = current_slice['Close'].iloc[-1]
            sma_50 = current_slice['Close'].rolling(50).mean().iloc[-1]
            sma_200 = current_slice['Close'].rolling(200).mean().iloc[-1]
            
            # Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¨Ø³Ø·
            if price > sma_200 and price > sma_50:
                signal = 1  # Ø´Ø±Ø§Ø¡
            elif price < sma_200 and price < sma_50:
                signal = -1  # Ø¨ÙŠØ¹
            else:
                signal = 0  # Ø§Ù†ØªØ¸Ø§Ø±
            
            signals.append(signal)
        
        return pd.Series(signals, index=data.index)
    
    def run_backtest(self, data: pd.DataFrame) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ"""
        results = []
        capital = self.initial_capital
        position = 0
        entry_price = 0
        
        for i, row in data.iterrows():
            current_price = row['Close']
            current_signal = row.get('position', 0)
            
            if current_signal == 1 and position == 0:  # Ø¥Ø´Ø§Ø±Ø© Ø´Ø±Ø§Ø¡
                # Ø¯Ø®ÙˆÙ„ ØµÙÙ‚Ø© Ø´Ø±Ø§Ø¡
                position = capital / current_price * (1 - self.commission)
                entry_price = current_price
                capital = 0
                action = 'BUY'
                
            elif current_signal == -1 and position > 0:  # Ø¥Ø´Ø§Ø±Ø© Ø¨ÙŠØ¹
                # Ø¥ØºÙ„Ø§Ù‚ ØµÙÙ‚Ø© Ø§Ù„Ø´Ø±Ø§Ø¡
                capital = position * current_price * (1 - self.commission)
                position = 0
                action = 'SELL'
                
            else:
                action = 'HOLD'
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ù„Ù…Ø­ÙØ¸Ø©
            portfolio_value = capital + (position * current_price if position > 0 else 0)
            
            results.append({
                'date': i,
                'price': current_price,
                'signal': current_signal,
                'action': action,
                'position': position,
                'capital': capital,
                'portfolio_value': portfolio_value,
                'return': (portfolio_value - self.initial_capital) / self.initial_capital * 100
            })
        
        return self._calculate_backtest_metrics(pd.DataFrame(results))
    
    def _calculate_backtest_metrics(self, results_df: pd.DataFrame) -> Dict:
        """Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        returns = results_df['return'].pct_change().dropna()
        final_return = results_df['return'].iloc[-1]
        
        # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        total_return = final_return
        annualized_return = (1 + final_return/100) ** (252/len(results_df)) - 1
        volatility = returns.std() * np.sqrt(252)
        sharpe_ratio = (annualized_return - 0.02) / volatility if volatility > 0 else 0
        
        # Ø£Ù‚ØµÙ‰ Ø§Ù†Ø®ÙØ§Ø¶ (Max Drawdown)
        cumulative = (1 + results_df['return']/100).cumprod()
        rolling_max = cumulative.expanding().max()
        drawdown = (cumulative - rolling_max) / rolling_max
        max_drawdown = drawdown.min()
        
        # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª
        signals = results_df['signal']
        trades = (signals != signals.shift()).sum() / 2
        
        # Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙÙˆØ²
        trade_returns = []
        in_trade = False
        entry_value = 0
        
        for _, row in results_df.iterrows():
            if row['action'] == 'BUY':
                in_trade = True
                entry_value = row['portfolio_value']
            elif row['action'] == 'SELL' and in_trade:
                trade_return = (row['portfolio_value'] - entry_value) / entry_value
                trade_returns.append(trade_return)
                in_trade = False
        
        win_rate = len([r for r in trade_returns if r > 0]) / len(trade_returns) if trade_returns else 0
        
        return {
            'total_return_percent': round(total_return, 2),
            'annualized_return_percent': round(annualized_return * 100, 2),
            'volatility_percent': round(volatility * 100, 2),
            'sharpe_ratio': round(sharpe_ratio, 2),
            'max_drawdown_percent': round(max_drawdown * 100, 2),
            'total_trades': int(trades),
            'win_rate_percent': round(win_rate * 100, 2),
            'final_portfolio_value': round(results_df['portfolio_value'].iloc[-1], 2),
            'best_trade': round(max(trade_returns) * 100, 2) if trade_returns else 0,
            'worst_trade': round(min(trade_returns) * 100, 2) if trade_returns else 0,
            'average_trade': round(np.mean(trade_returns) * 100, 2) if trade_returns else 0
        }

# =============================================================================
# ğŸ† Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# =============================================================================

class AdvancedGoldAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø°Ù‡Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ø´Ø§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    
    def __init__(self):
        # Ø±Ù…ÙˆØ² Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        self.symbols = {
            'gold': 'GC=F',          # Gold Futures
            'gold_etf': 'GLD',       # Gold ETF (backup)
            'silver': 'SI=F',        # Silver Futures
            'dxy': 'DX-Y.NYB',       # Dollar Index
            'vix': '^VIX',           # Volatility Index
            'treasury_10y': '^TNX',   # 10-Year Treasury
            'oil': 'CL=F',           # Oil Futures
            'spy': 'SPY',            # S&P 500 ETF
            'copper': 'HG=F',        # Copper (economic indicator)
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self.news_api_key = os.getenv("NEWS_API_KEY")
        self.sentiment_pipeline = None
        self.cache = DataCache("analyzer_cache")
        self.db_path = "gold_analysis_advanced.db"
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self._setup_advanced_database()
        self._load_sentiment_model()
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù„Ù„ÙŠÙ† Ø§Ù„Ù…ØªØ®ØµØµÙŠÙ†
        self.news_analyzer = None
        if self.sentiment_pipeline:
            self.news_analyzer = EnhancedNewsAnalyzer(self.news_api_key, self.sentiment_pipeline)
        
        self.backtest_engine = GoldBacktestEngine()
        
        logger.info("ğŸš€ Ù…Ø­Ù„Ù„ Ø§Ù„Ø°Ù‡Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„")
    
    def _setup_advanced_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS advanced_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp_utc TEXT NOT NULL,
                    signal TEXT NOT NULL,
                    signal_strength TEXT NOT NULL,
                    total_score REAL NOT NULL,
                    confidence_level REAL,
                    gold_price REAL,
                    execution_time_ms INTEGER,
                    
                    -- Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù†ØªÙŠØ¬Ø©
                    trend_score REAL,
                    momentum_score REAL,
                    correlation_score REAL,
                    news_score REAL,
                    volatility_score REAL,
                    seasonal_score REAL,
                    support_resistance_score REAL,
                    
                    -- Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚
                    dxy_value REAL,
                    vix_value REAL,
                    gold_silver_ratio REAL,
                    
                    -- Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ©
                    rsi_value REAL,
                    macd_signal TEXT,
                    bb_position TEXT,
                    
                    -- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±
                    stop_loss_price REAL,
                    take_profit_price REAL,
                    position_size_suggestion REAL,
                    
                    -- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
                    news_sentiment REAL,
                    news_confidence REAL,
                    news_articles_count INTEGER,
                    
                    -- Ø¨Ø§Ùƒ ØªÙŠØ³Øª
                    backtest_total_return REAL,
                    backtest_sharpe_ratio REAL,
                    backtest_max_drawdown REAL,
                    
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ÙØµÙ„
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS detailed_news (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id INTEGER,
                    headline TEXT,
                    source TEXT,
                    sentiment_score REAL,
                    confidence_score REAL,
                    relevance_score REAL,
                    matched_keywords TEXT,
                    published_at TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (analysis_id) REFERENCES advanced_analysis (id)
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø§Ùƒ ØªÙŠØ³Øª
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS backtest_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id INTEGER,
                    test_period_days INTEGER,
                    total_return REAL,
                    annualized_return REAL,
                    sharpe_ratio REAL,
                    max_drawdown REAL,
                    win_rate REAL,
                    total_trades INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (analysis_id) REFERENCES advanced_analysis (id)
                )
            ''')
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø§Ø±Ø³ Ù„Ù„Ø£Ø¯Ø§Ø¡
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON advanced_analysis(timestamp_utc)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_signal ON advanced_analysis(signal)')
            
            conn.commit()
            conn.close()
            logger.info("âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø¬Ø§Ù‡Ø²Ø©")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    
    def _load_sentiment_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            logger.info("ğŸ§  ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
            
            # ØªØ¬Ø±Ø¨Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="ProsusAI/finbert",
                tokenizer="ProsusAI/finbert",
                return_all_scores=True,
                device=-1  # Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU (Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©)
            )
            
            logger.info("âœ… Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¬Ø§Ù‡Ø²")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± - {e}")
            self.sentiment_pipeline = None
    
    async def fetch_comprehensive_market_data(self) -> Optional[pd.DataFrame]:
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø¨Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
        logger.info("ğŸ“Š Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø´Ø§Ù…Ù„Ø©...")
        
        # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        cache_key = f"market_data_{datetime.now().strftime('%Y%m%d_%H')}"
        cached_data = self.cache.get(cache_key, max_age_seconds=3600)  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
        
        if cached_data is not None:
            logger.info("ğŸ“¦ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹")
            return cached_data
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            symbols_list = list(self.symbols.values())
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… yfinance Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
            data = yf.download(
                symbols_list,
                period="18mo",  # Ø³Ù†Ø© ÙˆÙ†ØµÙ (ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§ÙÙŠØ© ÙˆØ§Ù„Ø³Ø±Ø¹Ø©)
                interval="1d",
                threads=True,
                progress=False,
                show_errors=False,
                repair=True  # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø·ÙˆØ¨Ø©
            )
            
            if data.empty:
                logger.warning("âš ï¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±ØºØ©ØŒ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ GLD ÙƒØ¨Ø¯ÙŠÙ„...")
                # ØªØ¬Ø±Ø¨Ø© GLD ÙƒØ¨Ø¯ÙŠÙ„
                self.symbols['gold'] = 'GLD'
                symbols_list[0] = 'GLD'
                data = yf.download(symbols_list, period="18mo", interval="1d", threads=True, progress=False)
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            gold_symbol = self.symbols['gold']
            required_columns = [('Close', gold_symbol), ('High', gold_symbol), ('Low', gold_symbol), ('Volume', gold_symbol)]
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                logger.error(f"âŒ Ø£Ø¹Ù…Ø¯Ø© Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_columns}")
                return None
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN ÙÙŠ Ø§Ù„Ø°Ù‡Ø¨
            data = data.dropna(subset=[('Close', gold_symbol)])
            
            if len(data) < 100:
                logger.error(f"âŒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©: {len(data)} ØµÙ ÙÙ‚Ø·")
                return None
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            self.cache.set(cache_key, data)
            
            logger.info(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(data)} ÙŠÙˆÙ… Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ({gold_symbol})")
            return data
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return None
    
    def calculate_comprehensive_technical_analysis(self, market_data: pd.DataFrame) -> pd.DataFrame:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…ØªØ®ØµØµØ©"""
        logger.info("ğŸ“ˆ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„...")
        
        try:
            gold_symbol = self.symbols['gold']
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ DataFrame Ù„Ù„Ø°Ù‡Ø¨
            gold_data = pd.DataFrame({
                'Open': market_data[('Open', gold_symbol)],
                'High': market_data[('High', gold_symbol)],
                'Low': market_data[('Low', gold_symbol)],
                'Close': market_data[('Close', gold_symbol)],
                'Volume': market_data[('Volume', gold_symbol)]
            }).dropna()
            
            # Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
            gold_data['SMA_10'] = ta.sma(gold_data['Close'], length=10)
            gold_data['SMA_20'] = ta.sma(gold_data['Close'], length=20)
            gold_data['SMA_50'] = ta.sma(gold_data['Close'], length=50)
            gold_data['SMA_100'] = ta.sma(gold_data['Close'], length=100)
            gold_data['SMA_200'] = ta.sma(gold_data['Close'], length=200)
            
            # Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ø£Ø³ÙŠØ©
            gold_data['EMA_12'] = ta.ema(gold_data['Close'], length=12)
            gold_data['EMA_26'] = ta.ema(gold_data['Close'], length=26)
            gold_data['EMA_50'] = ta.ema(gold_data['Close'], length=50)
            
            # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø²Ø®Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            gold_data['RSI'] = ta.rsi(gold_data['Close'], length=14)
            gold_data['RSI_SMA'] = ta.sma(gold_data['RSI'], length=5)  # ØªÙ†Ø¹ÙŠÙ… RSI
            
            # MACD Ø§Ù„Ù…Ø­Ø³Ù†
            macd_data = ta.macd(gold_data['Close'], fast=12, slow=26, signal=9)
            gold_data['MACD'] = macd_data['MACD_12_26_9']
            gold_data['MACD_Signal'] = macd_data['MACDs_12_26_9']
            gold_data['MACD_Histogram'] = macd_data['MACDh_12_26_9']
            
            # Bollinger Bands Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø¯Ø¯
            bb_20 = ta.bbands(gold_data['Close'], length=20, std=2)
            gold_data['BB_Upper_20'] = bb_20['BBU_20_2.0']
            gold_data['BB_Middle_20'] = bb_20['BBM_20_2.0']
            gold_data['BB_Lower_20'] = bb_20['BBL_20_2.0']
            gold_data['BB_Width'] = (gold_data['BB_Upper_20'] - gold_data['BB_Lower_20']) / gold_data['BB_Middle_20'] * 100
            gold_data['BB_Position'] = (gold_data['Close'] - gold_data['BB_Lower_20']) / (gold_data['BB_Upper_20'] - gold_data['BB_Lower_20']) * 100
            
            # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
            gold_data['ATR'] = ta.atr(gold_data['High'], gold_data['Low'], gold_data['Close'], length=14)
            gold_data['ATR_Percent'] = gold_data['ATR'] / gold_data['Close'] * 100
            
            # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¬Ù…
            gold_data['OBV'] = ta.obv(gold_data['Close'], gold_data['Volume'])
            gold_data['Volume_SMA'] = ta.sma(gold_data['Volume'], length=20)
            gold_data['Volume_Ratio'] = gold_data['Volume'] / gold_data['Volume_SMA']
            
            # Ù…Ø¤Ø´Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
            gold_data['Williams_R'] = ta.willr(gold_data['High'], gold_data['Low'], gold_data['Close'], length=14)
            gold_data['CCI'] = ta.cci(gold_data['High'], gold_data['Low'], gold_data['Close'], length=20)
            
            # Stochastic
            stoch = ta.stoch(gold_data['High'], gold_data['Low'], gold_data['Close'])
            gold_data['Stoch_K'] = stoch['STOCHk_14_3_3']
            gold_data['Stoch_D'] = stoch['STOCHd_14_3_3']
            
            # Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…ØªØ®ØµØµØ© Ø¨Ø§Ù„Ø°Ù‡Ø¨
            if self.symbols['silver'] in market_data.columns:
                silver_prices = market_data[('Close', self.symbols['silver'])]
                gold_data['Gold_Silver_Ratio'] = GoldSpecificIndicators.gold_silver_ratio(
                    gold_data['Close'], silver_prices
                )
            
            if self.symbols['oil'] in market_data.columns:
                oil_prices = market_data[('Close', self.symbols['oil'])]
                gold_data['Gold_Oil_Ratio'] = GoldSpecificIndicators.gold_oil_ratio(
                    gold_data['Close'], oil_prices
                )
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
            seasonal_analysis = GoldSpecificIndicators.seasonal_strength(gold_data['Close'])
            
            # Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©
            support_resistance = GoldSpecificIndicators.support_resistance_levels(gold_data['Close'])
            
            # Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
            volatility_regime = GoldSpecificIndicators.gold_volatility_regime(gold_data['Close'])
            
            # Ù…Ø­Ø§ÙƒØ§Ø© COT
            cot_simulation = GoldSpecificIndicators.cot_simulation(gold_data['Close'])
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ®ØµØµØ©
            gold_data = gold_data.assign(**{
                'Seasonal_Score': seasonal_analysis['seasonal_score'],
                'Volatility_Regime': volatility_regime['regime_score'],
                'Nearest_Resistance': support_resistance.get('nearest_resistance', gold_data['Close'].iloc[-1]),
                'Nearest_Support': support_resistance.get('nearest_support', gold_data['Close'].iloc[-1]),
                'COT_Commercial_Signal': 1 if cot_simulation['commercial_signal'] == 'buy' else -1 if cot_simulation['commercial_signal'] == 'sell' else 0
            })
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            gold_data = gold_data.dropna()
            
            logger.info(f"âœ… ØªÙ… Ø­Ø³Ø§Ø¨ {len(gold_data.columns)} Ù…Ø¤Ø´Ø±Ø§Ù‹ ÙÙ†ÙŠØ§Ù‹ - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©: {len(gold_data)} ØµÙ")
            
            # Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ø§Ø­Ù‚Ø§Ù‹
            self._seasonal_analysis = seasonal_analysis
            self._support_resistance = support_resistance
            self._volatility_regime = volatility_regime
            self._cot_simulation = cot_simulation
            
            return gold_data
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ: {e}")
            return pd.DataFrame()
    
    def calculate_advanced_scores(self, gold_data: pd.DataFrame, market_data: pd.DataFrame) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª"""
        logger.info("ğŸ¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©...")
        
        try:
            latest = gold_data.iloc[-1]
            current_price = latest['Close']
            scores = {}
            
            # 1. Ù†Ù‚Ø§Ø· Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù…Ø­Ø³Ù†Ø© (ÙˆØ²Ù†: 30%)
            trend_signals = 0
            if current_price > latest['SMA_200']: trend_signals += 3
            if current_price > latest['SMA_50']: trend_signals += 2
            if current_price > latest['SMA_20']: trend_signals += 1
            if latest['SMA_50'] > latest['SMA_200']: trend_signals += 1
            if latest['EMA_12'] > latest['EMA_26']: trend_signals += 1
            
            scores['trend'] = min(trend_signals / 8 * 4, 4) - 2  # ØªØ·Ø¨ÙŠØ¹ Ø¨ÙŠÙ† -2 Ùˆ +2
            
            # 2. Ù†Ù‚Ø§Ø· Ø§Ù„Ø²Ø®Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (ÙˆØ²Ù†: 25%)
            momentum_signals = 0
            
            # MACD
            if latest['MACD'] > latest['MACD_Signal']: momentum_signals += 1
            if latest['MACD_Histogram'] > gold_data['MACD_Histogram'].iloc[-2]: momentum_signals += 1
            
            # RSI
            rsi = latest['RSI']
            if 40 < rsi < 60: momentum_signals += 1  # Ù…Ù†Ø·Ù‚Ø© Ù…Ø­Ø§ÙŠØ¯Ø© ØµØ­ÙŠØ©
            elif 30 < rsi < 70: momentum_signals += 0.5
            elif rsi > 70: momentum_signals -= 0.5  # ØªØ´Ø¨Ø¹ Ø´Ø±Ø§Ø¡
            elif rsi < 30: momentum_signals += 1.5  # ØªØ´Ø¨Ø¹ Ø¨ÙŠØ¹ (ÙØ±ØµØ© Ø´Ø±Ø§Ø¡)
            
            # Williams %R
            if latest['Williams_R'] > -80: momentum_signals += 0.5
            
            # Stochastic
            if latest['Stoch_K'] > latest['Stoch_D']: momentum_signals += 0.5
            
            scores['momentum'] = (momentum_signals / 4.5 * 3) - 1.5  # ØªØ·Ø¨ÙŠØ¹ Ø¨ÙŠÙ† -1.5 Ùˆ +1.5
            
            # 3. Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª (ÙˆØ²Ù†: 20%)
            correlation_signals = 0
            
            # Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±
            try:
                dxy_current = market_data[('Close', self.symbols['dxy'])].iloc[-1]
                dxy_ma = market_data[('Close', self.symbols['dxy'])].rolling(20).mean().iloc[-1]
                if dxy_current < dxy_ma: correlation_signals += 1  # Ø¶Ø¹Ù Ø§Ù„Ø¯ÙˆÙ„Ø§Ø± ÙŠÙÙŠØ¯ Ø§Ù„Ø°Ù‡Ø¨
                if dxy_current < 105: correlation_signals += 0.5
            except:
                pass
            
            # Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ VIX
            try:
                vix_current = market_data[('Close', self.symbols['vix'])].iloc[-1]
                if vix_current > 20: correlation_signals += 1  # Ø§Ù„Ø®ÙˆÙ ÙŠÙÙŠØ¯ Ø§Ù„Ø°Ù‡Ø¨
                if vix_current > 30: correlation_signals += 1
            except:
                pass
            
            # Ù†Ø³Ø¨Ø© Ø§Ù„Ø°Ù‡Ø¨/Ø§Ù„ÙØ¶Ø©
            if 'Gold_Silver_Ratio' in latest:
                gsr = latest['Gold_Silver_Ratio']
                if 70 < gsr < 90: correlation_signals += 0.5  # Ù†Ø·Ø§Ù‚ Ø·Ø¨ÙŠØ¹ÙŠ
                elif gsr > 90: correlation_signals += 1  # Ø§Ù„Ø°Ù‡Ø¨ Ù…Ù‚ÙˆÙ… Ø¨Ø£Ø¹Ù„Ù‰ Ù…Ù† Ù‚ÙŠÙ…ØªÙ‡
            
            scores['correlation'] = (correlation_signals / 4.5 * 2) - 1  # Ø¨ÙŠÙ† -1 Ùˆ +1
            
            # 4. Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª ÙˆØ§Ù„Ø³ÙˆÙ‚ (ÙˆØ²Ù†: 15%)
            volatility_score = latest.get('Volatility_Regime', 0)
            
            # Bollinger Bands
            bb_position = latest.get('BB_Position', 50)
            if bb_position < 20: volatility_score += 1  # Ù‚Ø±Ø¨ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø³ÙÙ„ÙŠ
            elif bb_position > 80: volatility_score -= 0.5  # Ù‚Ø±Ø¨ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø¹Ù„ÙˆÙŠ
            
            # ATR
            atr_percent = latest.get('ATR_Percent', 2)
            if atr_percent > 3: volatility_score += 0.5  # ØªÙ‚Ù„Ø¨Ø§Øª Ø¹Ø§Ù„ÙŠØ© ØªÙÙŠØ¯ Ø§Ù„Ø°Ù‡Ø¨
            
            scores['volatility'] = min(max(volatility_score, -1), 1)
            
            # 5. Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ© (ÙˆØ²Ù†: 5%)
            scores['seasonal'] = latest.get('Seasonal_Score', 0)
            
            # 6. Ù†Ù‚Ø§Ø· Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© (ÙˆØ²Ù†: 5%)
            try:
                resistance_distance = self._support_resistance.get('resistance_distance', 10)
                support_distance = self._support_resistance.get('support_distance', 10)
                
                if support_distance and support_distance < 2:  # Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ø¯Ø¹Ù…
                    support_resistance_score = 1
                elif resistance_distance and resistance_distance < 2:  # Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©
                    support_resistance_score = -0.5
                else:
                    support_resistance_score = 0
                    
                scores['support_resistance'] = support_resistance_score
            except:
                scores['support_resistance'] = 0
            
            logger.info("âœ… ØªÙ… Ø­Ø³Ø§Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
            return scores
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·: {e}")
            return {
                'trend': 0, 'momentum': 0, 'correlation': 0, 
                'volatility': 0, 'seasonal': 0, 'support_resistance': 0
            }
    
    async def run_ultimate_analysis(self) -> Dict:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
        start_time = time.time()
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ø°Ù‡Ø¨...")
        
        try:
            # 1. Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚
            market_data = await self.fetch_comprehensive_market_data()
            if market_data is None:
                return {"status": "error", "error": "ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚"}
            
            # 2. Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„
            gold_data = self.calculate_comprehensive_technical_analysis(market_data)
            if gold_data.empty:
                return {"status": "error", "error": "ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ"}
            
            # 3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØªØ·ÙˆØ± (Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ)
            news_analysis_task = None
            if self.news_analyzer:
                news_analysis_task = asyncio.create_task(self.news_analyzer.run_enhanced_analysis())
            
            # 4. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            scores = self.calculate_advanced_scores(gold_data, market_data)
            
            # 5. Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
            if news_analysis_task:
                news_analysis = await news_analysis_task
            else:
                news_analysis = {"status": "skipped", "news_score": 0, "confidence_level": 0, "headlines": []}
            
            # 6. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø§Ùƒ ØªÙŠØ³Øª
            logger.info("ğŸ”¬ ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± ØªØ§Ø±ÙŠØ®ÙŠ...")
            backtest_data = self.backtest_engine.prepare_backtest_data(market_data, gold_data)
            backtest_results = self.backtest_engine.run_backtest(backtest_data)
            
            # 7. Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ©
            vix_current = market_data[('Close', self.symbols['vix'])].iloc[-1] if ('Close', self.symbols['vix']) in market_data.columns else 20
            
            adaptive_weights = self._calculate_adaptive_weights(vix_current, scores, backtest_results)
            
            # 8. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            final_score = sum(scores[component] * weight for component, weight in adaptive_weights.items() if component in scores)
            
            # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
            news_weight = 0.10
            news_contribution = news_analysis.get('news_score', 0) * news_weight * 2
            final_score += news_contribution
            
            # 9. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© ÙˆÙ‚ÙˆØªÙ‡Ø§
            signal_info = self._determine_advanced_signal(final_score, backtest_results, news_analysis)
            
            # 10. Ø­Ø³Ø§Ø¨ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±
            latest = gold_data.iloc[-1]
            current_price = latest['Close']
            risk_management = self._calculate_risk_management(current_price, latest['ATR'], signal_info['signal'], latest)
            
            # 11. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            execution_time = round((time.time() - start_time) * 1000)  # Ø¨Ø§Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©
            
            comprehensive_result = {
                "timestamp_utc": datetime.utcnow().isoformat(),
                "execution_time_ms": execution_time,
                "status": "success",
                
                # Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                "signal": signal_info['signal'],
                "signal_strength": signal_info['strength'],
                "confidence_level": signal_info['confidence'],
                "total_score": round(final_score, 3),
                
                # Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
                "technical_scores": {k: round(v, 3) for k, v in scores.items()},
                "adaptive_weights": adaptive_weights,
                "news_contribution": round(news_contribution, 3),
                
                # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
                "market_data": {
                    "gold_price": round(current_price, 2),
                    "dxy": round(market_data[('Close', self.symbols['dxy'])].iloc[-1], 2) if ('Close', self.symbols['dxy']) in market_data.columns else 0,
                    "vix": round(vix_current, 2),
                    "gold_silver_ratio": round(latest.get('Gold_Silver_Ratio', 0), 2),
                    "gold_oil_ratio": round(latest.get('Gold_Oil_Ratio', 0), 2),
                },
                
                # Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                "technical_indicators": {
                    "rsi": round(latest['RSI'], 2),
                    "macd_signal": "bullish" if latest['MACD'] > latest['MACD_Signal'] else "bearish",
                    "bb_position": round(latest.get('BB_Position', 50), 1),
                    "atr_percent": round(latest.get('ATR_Percent', 2), 3),
                    "volume_ratio": round(latest.get('Volume_Ratio', 1), 2),
                    "williams_r": round(latest['Williams_R'], 2),
                    "cci": round(latest['CCI'], 2)
                },
                
                # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ®ØµØµ Ø¨Ø§Ù„Ø°Ù‡Ø¨
                "gold_specific_analysis": {
                    "seasonal_analysis": getattr(self, '_seasonal_analysis', {}),
                    "support_resistance": getattr(self, '_support_resistance', {}),
                    "volatility_regime": getattr(self, '_volatility_regime', {}),
                    "cot_simulation": getattr(self, '_cot_simulation', {})
                },
                
                # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±
                "risk_management": risk_management,
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
                "news_analysis": news_analysis,
                
                # Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø§Ùƒ ØªÙŠØ³Øª
                "backtest_results": backtest_results,
                
                # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
                "performance_info": {
                    "data_points_analyzed": len(gold_data),
                    "indicators_calculated": len(gold_data.columns),
                    "news_articles_processed": len(news_analysis.get('headlines', [])),
                    "backtest_period_days": len(backtest_data),
                    "cache_hits": "market_data" if self.cache.get(f"market_data_{datetime.now().strftime('%Y%m%d_%H')}") else "none"
                }
            }
            
            # 12. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self._save_comprehensive_results(comprehensive_result)
            
            logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ ÙÙŠ {execution_time}ms")
            logger.info(f"ğŸ“Š Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {signal_info['signal']} ({signal_info['strength']}) - Ø§Ù„Ù†ØªÙŠØ¬Ø©: {final_score:.3f}")
            
            return comprehensive_result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„: {e}")
            return {
                "status": "error", 
                "error": str(e),
                "execution_time_ms": round((time.time() - start_time) * 1000)
            }
    
    def _calculate_adaptive_weights(self, vix_value: float, scores: Dict, backtest_results: Dict) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØªÙƒÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¸Ø±ÙˆÙ Ø§Ù„Ø³ÙˆÙ‚ ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ"""
        base_weights = {
            'trend': 0.30,
            'momentum': 0.25, 
            'correlation': 0.20,
            'volatility': 0.15,
            'seasonal': 0.05,
            'support_resistance': 0.05
        }
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø­Ø³Ø¨ VIX
        if vix_value > 30:  # Ø³ÙˆÙ‚ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØªÙ‚Ù„Ø¨
            base_weights['volatility'] += 0.05
            base_weights['correlation'] += 0.05
            base_weights['trend'] -= 0.05
            base_weights['momentum'] -= 0.05
        elif vix_value < 15:  # Ø³ÙˆÙ‚ Ù‡Ø§Ø¯Ø¦
            base_weights['trend'] += 0.05
            base_weights['momentum'] += 0.05
            base_weights['volatility'] -= 0.05
            base_weights['correlation'] -= 0.05
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø§Ùƒ ØªÙŠØ³Øª
        if backtest_results.get('sharpe_ratio', 0) > 1:
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¬ÙŠØ¯Ø©ØŒ Ø²ÙŠØ§Ø¯Ø© ÙˆØ²Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù‚ÙˆÙŠØ©
            for component, score in scores.items():
                if abs(score) > 1 and component in base_weights:
                    base_weights[component] += 0.02
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        total_weight = sum(base_weights.values())
        return {k: round(v/total_weight, 3) for k, v in base_weights.items()}
    
    def _determine_advanced_signal(self, final_score: float, backtest_results: Dict, news_analysis: Dict) -> Dict:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©"""
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©
        confidence_factors = []
        
        # Ø«Ù‚Ø© Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙÙ†ÙŠØ©
        confidence_factors.append(min(abs(final_score) / 2, 1))
        
        # Ø«Ù‚Ø© Ù…Ù† Ø§Ù„Ø¨Ø§Ùƒ ØªÙŠØ³Øª
        sharpe = backtest_results.get('sharpe_ratio', 0)
        win_rate = backtest_results.get('win_rate_percent', 50) / 100
        confidence_factors.append(min(max(sharpe, 0) / 2, 1))
        confidence_factors.append(win_rate)
        
        # Ø«Ù‚Ø© Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        news_confidence = news_analysis.get('confidence_level', 0)
        confidence_factors.append(news_confidence)
        
        overall_confidence = np.mean(confidence_factors)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
        if final_score >= 2.0:
            signal, strength = "Buy", "Very Strong Buy"
        elif final_score >= 1.5:
            signal, strength = "Buy", "Strong Buy" 
        elif final_score >= 1.0:
            signal, strength = "Buy", "Buy"
        elif final_score >= 0.5:
            signal, strength = "Buy", "Weak Buy"
        elif final_score <= -2.0:
            signal, strength = "Sell", "Very Strong Sell"
        elif final_score <= -1.5:
            signal, strength = "Sell", "Strong Sell"
        elif final_score <= -1.0:
            signal, strength = "Sell", "Sell"
        elif final_score <= -0.5:
            signal, strength = "Sell", "Weak Sell"
        else:
            signal, strength = "Hold", "Hold"
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ© Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©
        if overall_confidence < 0.5:
            if "Very Strong" in strength:
                strength = strength.replace("Very Strong", "Strong")
            elif "Strong" in strength and "Very" not in strength:
                strength = strength.replace("Strong", "")
        
        return {
            'signal': signal,
            'strength': strength,
            'confidence': round(overall_confidence, 3)
        }
    
    def _calculate_risk_management(self, current_price: float, atr: float, signal: str, latest_data: pd.Series) -> Dict:
        """Ø­Ø³Ø§Ø¨ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        
        risk_management = {}
        
        # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        if 'buy' in signal.lower():
            stop_loss = current_price - (2.5 * atr)
            take_profit = current_price + (4 * atr)  # Ù†Ø³Ø¨Ø© Ù…Ø®Ø§Ø·Ø±Ø© 1:1.6
        elif 'sell' in signal.lower():
            stop_loss = current_price + (2.5 * atr)
            take_profit = current_price - (4 * atr)
        else:
            stop_loss = current_price
            take_profit = current_price
        
        # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ù…Ù‚ØªØ±Ø­ (2% Ù…Ø®Ø§Ø·Ø±Ø©)
        risk_amount = 0.02  # 2% Ù…Ù† Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„
        price_risk = abs(current_price - stop_loss)
        if price_risk > 0:
            position_size_percent = risk_amount / (price_risk / current_price)
        else:
            position_size_percent = 0.1  # Ø§ÙØªØ±Ø§Ø¶ÙŠ 10%
        
        risk_management = {
            'stop_loss_price': round(stop_loss, 2),
            'take_profit_price': round(take_profit, 2),
            'position_size_percent': round(min(position_size_percent * 100, 25), 2),  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 25%
            'risk_reward_ratio': round(abs(take_profit - current_price) / abs(current_price - stop_loss), 2) if abs(current_price - stop_loss) > 0 else 0,
            'atr_based_stop': True,
            'volatility_adjustment': round(latest_data.get('ATR_Percent', 2), 3)
        }
        
        return risk_management
    
    def _save_comprehensive_results(self, result: Dict) -> int:
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            cursor.execute('''
                INSERT INTO advanced_analysis (
                    timestamp_utc, signal, signal_strength, total_score, confidence_level,
                    gold_price, execution_time_ms, trend_score, momentum_score,
                    correlation_score, news_score, volatility_score, seasonal_score,
                    support_resistance_score, dxy_value, vix_value, gold_silver_ratio,
                    rsi_value, macd_signal, bb_position, stop_loss_price,
                    take_profit_price, position_size_suggestion, news_sentiment,
                    news_confidence, news_articles_count, backtest_total_return,
                    backtest_sharpe_ratio, backtest_max_drawdown
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                result['timestamp_utc'], result['signal'], result['signal_strength'],
                result['total_score'], result['confidence_level'],
                result['market_data']['gold_price'], result['execution_time_ms'],
                result['technical_scores']['trend'], result['technical_scores']['momentum'],
                result['technical_scores']['correlation'], result['technical_scores'].get('news', 0),
                result['technical_scores']['volatility'], result['technical_scores']['seasonal'],
                result['technical_scores']['support_resistance'],
                result['market_data']['dxy'], result['market_data']['vix'],
                result['market_data']['gold_silver_ratio'],
                result['technical_indicators']['rsi'], result['technical_indicators']['macd_signal'],
                result['technical_indicators']['bb_position'],
                result['risk_management']['stop_loss_price'],
                result['risk_management']['take_profit_price'],
                result['risk_management']['position_size_percent'],
                result['news_analysis'].get('news_score', 0),
                result['news_analysis'].get('confidence_level', 0),
                len(result['news_analysis'].get('headlines', [])),
                result['backtest_results']['total_return_percent'],
                result['backtest_results']['sharpe_ratio'],
                result['backtest_results']['max_drawdown_percent']
            ))
            
            analysis_id = cursor.lastrowid
            
            # Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
            for headline in result['news_analysis'].get('headlines', []):
                cursor.execute('''
                    INSERT INTO detailed_news 
                    (analysis_id, headline, source, sentiment_score, confidence_score, 
                     relevance_score, matched_keywords, published_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    analysis_id, headline.get('title', ''), headline.get('source', ''),
                    headline.get('sentiment_score', 0), headline.get('confidence', 0),
                    headline.get('relevance_score', 0), 
                    json.dumps(headline.get('matched_keywords', [])),
                    headline.get('published_at', '')
                ))
            
            # Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø§Ùƒ ØªÙŠØ³Øª
            cursor.execute('''
                INSERT INTO backtest_results 
                (analysis_id, test_period_days, total_return, annualized_return,
                 sharpe_ratio, max_drawdown, win_rate, total_trades)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                analysis_id, result['performance_info']['backtest_period_days'],
                result['backtest_results']['total_return_percent'],
                result['backtest_results']['annualized_return_percent'],
                result['backtest_results']['sharpe_ratio'],
                result['backtest_results']['max_drawdown_percent'],
                result['backtest_results']['win_rate_percent'],
                result['backtest_results']['total_trades']
            ))
            
            conn.commit()
            conn.close()
            
            # Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù JSON
            with open("gold_analysis_ultimate.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ - ID: {analysis_id}")
            return analysis_id
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {e}")
            return -1

# =============================================================================
# ğŸ¯ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# =============================================================================

async def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        analyzer = AdvancedGoldAnalyzer()
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
        results = await analyzer.run_ultimate_analysis()
        
        if results.get("status") == "error":
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {results.get('error')}")
            return results
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("\n" + "="*80)
        print("ğŸ† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ø°Ù‡Ø¨")
        print("="*80)
        print(f"â±ï¸  ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {results['execution_time_ms']}ms")
        print(f"ğŸ¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©: {results['signal']} ({results['signal_strength']})")
        print(f"ğŸ“Š Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {results['total_score']}")
        print(f"ğŸ”’ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {results['confidence_level']:.1%}")
        print(f"ğŸ’° Ø³Ø¹Ø± Ø§Ù„Ø°Ù‡Ø¨: ${results['market_data']['gold_price']}")
        print(f"ğŸ›‘ ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø©: ${results['risk_management']['stop_loss_price']}")
        print(f"ğŸ¯ Ø¬Ù†ÙŠ Ø§Ù„Ø£Ø±Ø¨Ø§Ø­: ${results['risk_management']['take_profit_price']}")
        print(f"ğŸ“ Ø­Ø¬Ù… Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ù…Ù‚ØªØ±Ø­: {results['risk_management']['position_size_percent']:.1f}%")
        
        print(f"\nğŸ“ˆ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        for component, score in results['technical_scores'].items():
            print(f"  â€¢ {component.replace('_', ' ').title()}: {score:.3f}")
        
        print(f"\nğŸ“Š Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:")
        tech_indicators = results['technical_indicators']
        print(f"  â€¢ RSI: {tech_indicators['rsi']}")
        print(f"  â€¢ MACD: {tech_indicators['macd_signal']}")
        print(f"  â€¢ Bollinger Bands: {tech_indicators['bb_position']:.1f}%")
        
        print(f"\nğŸ”¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø§Ùƒ ØªÙŠØ³Øª:")
        bt = results['backtest_results']
        print(f"  â€¢ Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {bt['total_return_percent']:.2f}%")
        print(f"  â€¢ Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨: {bt['sharpe_ratio']:.2f}")
        print(f"  â€¢ Ø£Ù‚ØµÙ‰ Ø§Ù†Ø®ÙØ§Ø¶: {bt['max_drawdown_percent']:.2f}%")
        print(f"  â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙÙˆØ²: {bt['win_rate_percent']:.1f}%")
        
        print(f"\nğŸ“° ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:")
        news = results['news_analysis']
        print(f"  â€¢ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„: {news['status']}")
        print(f"  â€¢ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {news.get('news_score', 0):.3f}")
        print(f"  â€¢ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {news.get('confidence_level', 0):.3f}")
        print(f"  â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª: {len(news.get('headlines', []))}")
        
        if news.get('headlines'):
            print(f"\nğŸ“‹ Ø£Ù‡Ù… Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†:")
            for i, headline in enumerate(news['headlines'][:5], 1):
                print(f"  {i}. {headline['title'][:70]}... [{headline['source']}]")
        
        print(f"\nğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ:")
        print("  â€¢ gold_analysis_ultimate.json")
        print("  â€¢ gold_analysis_advanced.db")
        print("  â€¢ gold_analysis_advanced.log")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: {e}")
        return {"status": "error", "error": str(e)}

if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
    results = asyncio.run(main())
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„Ø§Ø­Ù‚Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
    if results and results.get("status") != "error":
        logger.info("ğŸ‰ ØªÙ… Ø¥Ù†Ø¬Ø§Ø² Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")
    else:
        logger.error("ğŸ’¥ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„ØªØ­Ù„ÙŠÙ„")
        exit(1)