#!/usr/bin/env python3
import yfinance as yf
import pandas as pd
import numpy as np
import requests
import json
import os
import sqlite3
import joblib
from datetime import datetime, timedelta
import warnings
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb
from textblob import TextBlob
import spacy
import backtrader as bt
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
import aiohttp

warnings.filterwarnings('ignore')

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ spaCy Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
try:
    nlp = spacy.load("en_core_web_sm")
except:
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

class MLPredictor:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = []
        self.model_path = "gold_ml_model.pkl"
        self.scaler_path = "gold_scaler.pkl"
        
    def prepare_features(self, analysis_data):
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        features = {}
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„
        if 'gold_analysis' in analysis_data:
            scores = analysis_data['gold_analysis'].get('component_scores', {})
            features.update({f'score_{k}': v for k, v in scores.items()})
            features['total_score'] = analysis_data['gold_analysis'].get('total_score', 0)
            
            # Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©
            tech_summary = analysis_data['gold_analysis'].get('technical_summary', {})
            features.update({f'tech_{k}': v for k, v in tech_summary.items()})
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¬Ù…
        if 'volume_analysis' in analysis_data:
            vol = analysis_data['volume_analysis']
            features['volume_ratio'] = vol.get('volume_ratio', 1)
            features['volume_strength_encoded'] = self._encode_volume_strength(vol.get('volume_strength', 'Ø·Ø¨ÙŠØ¹ÙŠ'))
        
        # Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
        if 'market_correlations' in analysis_data:
            corr = analysis_data['market_correlations'].get('correlations', {})
            features.update({f'corr_{k}': v for k, v in corr.items()})
        
        # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©
        if 'economic_data' in analysis_data:
            features['economic_score'] = analysis_data['economic_data'].get('score', 0)
        
        # Ù…Ø³ØªÙˆÙŠØ§Øª ÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ
        if 'fibonacci_levels' in analysis_data:
            fib = analysis_data['fibonacci_levels']
            features['fib_position'] = fib.get('current_position', 50)
        
        return features
    
    def _encode_volume_strength(self, strength):
        """ØªØ­ÙˆÙŠÙ„ Ù‚ÙˆØ© Ø§Ù„Ø­Ø¬Ù… Ø¥Ù„Ù‰ Ø±Ù‚Ù…"""
        mapping = {
            'Ø¶Ø¹ÙŠÙ': 0,
            'Ø·Ø¨ÙŠØ¹ÙŠ': 1,
            'Ù‚ÙˆÙŠ': 2,
            'Ù‚ÙˆÙŠ Ø¬Ø¯Ø§Ù‹': 3
        }
        return mapping.get(strength, 1)
    
    def train_model(self, historical_data):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ"""
        print("ğŸ¤– Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X = []
        y = []
        
        for record in historical_data:
            features = self.prepare_features(record['analysis'])
            if features:
                X.append(list(features.values()))
                # Ø§Ù„Ù‡Ø¯Ù: Ù‡Ù„ Ø§Ø±ØªÙØ¹ Ø§Ù„Ø³Ø¹Ø± Ø¨Ù†Ø³Ø¨Ø© 1% Ø®Ù„Ø§Ù„ 5 Ø£ÙŠØ§Ù…ØŸ
                y.append(1 if record['price_change_5d'] > 1.0 else 0)
        
        if len(X) < 100:
            print("âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
            return False
        
        X = np.array(X)
        y = np.array(y)
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø©
        models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
            'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False)
        }
        
        best_model = None
        best_score = 0
        
        for name, model in models.items():
            print(f"ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {name}...")
            model.fit(X_train_scaled, y_train)
            
            # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            
            print(f"  - Ø§Ù„Ø¯Ù‚Ø©: {accuracy:.2%}")
            print(f"  - Precision: {precision:.2%}")
            print(f"  - Recall: {recall:.2%}")
            print(f"  - F1 Score: {f1:.2%}")
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
            if f1 > best_score:
                best_score = f1
                best_model = model
                self.model = model
        
        print(f"\nâœ… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {type(best_model).__name__} Ù…Ø¹ F1 Score: {best_score:.2%}")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        joblib.dump(self.model, self.model_path)
        joblib.dump(self.scaler, self.scaler_path)
        
        return True
    
    def predict_probability(self, analysis_data):
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…Ø­Ù…Ù„Ø§Ù‹
            if self.model is None:
                if os.path.exists(self.model_path):
                    self.model = joblib.load(self.model_path)
                    self.scaler = joblib.load(self.scaler_path)
                else:
                    return None, "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø±Ø¨ Ø¨Ø¹Ø¯"
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª
            features = self.prepare_features(analysis_data)
            X = np.array([list(features.values())])
            
            # Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤
            X_scaled = self.scaler.transform(X)
            probability = self.model.predict_proba(X_scaled)[0][1]
            
            # ØªÙØ³ÙŠØ± Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
            if probability > 0.75:
                interpretation = "Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ù†Ø¬Ø§Ø­"
            elif probability > 0.60:
                interpretation = "Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¬ÙŠØ¯Ø© Ù„Ù„Ù†Ø¬Ø§Ø­"
            elif probability > 0.45:
                interpretation = "Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù…ØªÙˆØ³Ø·Ø© - Ø­Ø°Ø±"
            else:
                interpretation = "Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù…Ù†Ø®ÙØ¶Ø© - ØªØ¬Ù†Ø¨"
            
            return probability, interpretation
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return None, str(e)

class MultiTimeframeAnalyzer:
    """Ù…Ø­Ù„Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
    
    def __init__(self):
        self.timeframes = {
            '1h': {'period': '5d', 'weight': 0.2},
            '4h': {'period': '1mo', 'weight': 0.3},
            '1d': {'period': '3mo', 'weight': 0.5}
        }
    
    def analyze_timeframe(self, symbol, interval, period):
        """ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø± Ø²Ù…Ù†ÙŠ ÙˆØ§Ø­Ø¯"""
        try:
            data = yf.download(symbol, period=period, interval=interval, progress=False)
            if data.empty:
                return None
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            data['SMA_20'] = data['Close'].rolling(20).mean()
            data['RSI'] = self._calculate_rsi(data['Close'])
            
            # Ø­Ø³Ø§Ø¨ MACD
            exp1 = data['Close'].ewm(span=12).mean()
            exp2 = data['Close'].ewm(span=26).mean()
            data['MACD'] = exp1 - exp2
            data['MACD_Signal'] = data['MACD'].ewm(span=9).mean()
            
            latest = data.iloc[-1]
            
            # Ù†Ø¸Ø§Ù… Ù†Ù‚Ø§Ø· Ù…Ø¨Ø³Ø·
            score = 0
            
            # Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            if latest['Close'] > latest['SMA_20']:
                score += 1
            else:
                score -= 1
            
            # RSI
            if 30 <= latest['RSI'] <= 70:
                if latest['RSI'] > 50:
                    score += 0.5
                else:
                    score -= 0.5
            elif latest['RSI'] < 30:
                score += 1  # Ø°Ø±ÙˆØ© Ø¨ÙŠØ¹
            else:
                score -= 1  # Ø°Ø±ÙˆØ© Ø´Ø±Ø§Ø¡
            
            # MACD
            if latest['MACD'] > latest['MACD_Signal']:
                score += 1
            else:
                score -= 1
            
            return {
                'score': score,
                'trend': 'ØµØ§Ø¹Ø¯' if score > 0 else 'Ù‡Ø§Ø¨Ø·',
                'strength': abs(score),
                'rsi': latest['RSI'],
                'price': latest['Close']
            }
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ {interval}: {e}")
            return None
    
    def _calculate_rsi(self, prices, period=14):
        """Ø­Ø³Ø§Ø¨ RSI"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def get_coherence_score(self, symbol):
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙˆØ§ÙÙ‚ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        print("â° ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©...")
        
        results = {}
        total_weighted_score = 0
        total_weight = 0
        
        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø¥Ø·Ø§Ø± Ø²Ù…Ù†ÙŠ
        for tf_name, tf_config in self.timeframes.items():
            if tf_name == '1h':
                interval = '1h'
            elif tf_name == '4h':
                interval = '4h'
            else:
                interval = '1d'
            
            analysis = self.analyze_timeframe(symbol, interval, tf_config['period'])
            
            if analysis:
                results[tf_name] = analysis
                total_weighted_score += analysis['score'] * tf_config['weight']
                total_weight += tf_config['weight']
        
        if total_weight == 0:
            return 0, results
        
        # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙˆØ§ÙÙ‚
        coherence_score = total_weighted_score / total_weight
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§ÙÙ‚
        trends = [r['trend'] for r in results.values() if r]
        if all(t == 'ØµØ§Ø¹Ø¯' for t in trends):
            coherence_score += 2  # Ù…ÙƒØ§ÙØ£Ø© Ù„Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„ÙƒØ§Ù…Ù„
            coherence_analysis = "ØªÙˆØ§ÙÙ‚ ÙƒØ§Ù…Ù„ ØµØ§Ø¹Ø¯ - Ù‚ÙˆØ© Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ©"
        elif all(t == 'Ù‡Ø§Ø¨Ø·' for t in trends):
            coherence_score -= 2  # Ø¹Ù‚ÙˆØ¨Ø© Ù„Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù‡Ø§Ø¨Ø·
            coherence_analysis = "ØªÙˆØ§ÙÙ‚ ÙƒØ§Ù…Ù„ Ù‡Ø§Ø¨Ø· - Ø¶Ø¹Ù Ø´Ø¯ÙŠØ¯"
        elif len(set(trends)) > 1:
            coherence_analysis = "ØªØ¶Ø§Ø±Ø¨ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ© - Ø­Ø°Ø±"
        else:
            coherence_analysis = "ØªÙˆØ§ÙÙ‚ Ø¬Ø²Ø¦ÙŠ"
        
        return coherence_score, {
            'timeframes': results,
            'coherence_score': round(coherence_score, 2),
            'analysis': coherence_analysis,
            'recommendation': self._get_mtf_recommendation(coherence_score)
        }
    
    def _get_mtf_recommendation(self, score):
        """ØªÙˆØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§ÙÙ‚"""
        if score > 2:
            return "Ø¯Ø®ÙˆÙ„ Ù‚ÙˆÙŠ - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø·Ø± Ù…ØªÙˆØ§ÙÙ‚Ø©"
        elif score > 1:
            return "Ø¯Ø®ÙˆÙ„ Ù…Ø¹ØªØ¯Ù„ - ØªÙˆØ§ÙÙ‚ Ø¬ÙŠØ¯"
        elif score > -1:
            return "Ø§Ù†ØªØ¸Ø§Ø± - Ø¹Ø¯Ù… ÙˆØ¶ÙˆØ­"
        elif score > -2:
            return "ØªØ¬Ù†Ø¨ Ø§Ù„Ø´Ø±Ø§Ø¡ - Ø¶Ø¹Ù"
        else:
            return "Ø¨ÙŠØ¹ Ø£Ùˆ ØªØ¬Ù†Ø¨ ÙƒØ§Ù…Ù„ - ØªÙˆØ§ÙÙ‚ Ù‡Ø§Ø¨Ø·"

class AdvancedNewsAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø£Ø®Ø¨Ø§Ø± Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«"""
    
    def __init__(self, api_key):
        self.api_key = api_key
        self.event_patterns = {
            'interest_rate': {
                'keywords': ['interest rate', 'rate decision', 'fomc', 'federal reserve', 'fed meeting'],
                'entities': ['Federal Reserve', 'Fed', 'FOMC', 'Jerome Powell'],
                'impact_multiplier': 3
            },
            'inflation': {
                'keywords': ['inflation', 'cpi', 'consumer price', 'pce'],
                'entities': ['Bureau of Labor Statistics', 'BLS'],
                'impact_multiplier': 2.5            },
            'employment': {
                'keywords': ['employment', 'jobs', 'nfp', 'non-farm payroll', 'unemployment'],
                'entities': ['Labor Department', 'BLS'],
                'impact_multiplier': 2
            },
            'geopolitical': {
                'keywords': ['war', 'conflict', 'sanctions', 'crisis', 'tension'],
                'entities': ['Russia', 'China', 'Middle East', 'Ukraine'],
                'impact_multiplier': 2.5
            },
            'central_bank': {
                'keywords': ['central bank', 'ecb', 'boe', 'boj', 'monetary policy'],
                'entities': ['ECB', 'Bank of England', 'Bank of Japan'],
                'impact_multiplier': 2
            },
            'dollar': {
                'keywords': ['dollar', 'dxy', 'usd', 'currency'],
                'entities': ['Dollar Index', 'DXY'],
                'impact_multiplier': 1.5
            }
        }
    
    def extract_events(self, articles):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NLP"""
        extracted_events = []
        
        for article in articles:
            if not article.get('title'):
                continue
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„ÙˆØµÙ
            text = f"{article['title']} {article.get('description', '')}"
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… spaCy
            doc = nlp(text)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
            entities = [(ent.text, ent.label_) for ent in doc.ents]
            
            # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø­Ø¯Ø«
            event_type = self._classify_event(text.lower(), entities)
            
            if event_type:
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
                sentiment_score = self._advanced_sentiment_analysis(text)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
                numbers = self._extract_numbers(doc)
                
                event = {
                    'type': event_type,
                    'title': article['title'],
                    'source': article.get('source', {}).get('name', 'Unknown'),
                    'published': article.get('publishedAt', ''),
                    'entities': entities,
                    'numbers': numbers,
                    'sentiment_score': sentiment_score,
                    'impact_score': self._calculate_impact_score(event_type, sentiment_score),
                    'url': article.get('url', '')
                }
                
                extracted_events.append(event)
        
        return self._analyze_events_impact(extracted_events)
    
    def _classify_event(self, text, entities):
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø­Ø¯Ø«"""
        for event_type, patterns in self.event_patterns.items():
            # ÙØ­Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            if any(keyword in text for keyword in patterns['keywords']):
                return event_type
            
            # ÙØ­Øµ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
            entity_texts = [ent[0] for ent in entities]
            if any(entity in ' '.join(entity_texts) for entity in patterns['entities']):
                return event_type
        
        return None
    
    def _advanced_sentiment_analysis(self, text):
        """ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø´Ø§Ø¹Ø±"""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… TextBlob Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        blob = TextBlob(text)
        basic_sentiment = blob.sentiment.polarity
        
        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ø°Ù‡Ø¨
        gold_positive = ['surge', 'rally', 'gain', 'rise', 'bullish', 'support', 'demand']
        gold_negative = ['fall', 'drop', 'decline', 'bearish', 'pressure', 'weak']
        
        positive_count = sum(1 for word in gold_positive if word in text.lower())
        negative_count = sum(1 for word in gold_negative if word in text.lower())
        
        # Ø¯Ù…Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
        final_sentiment = basic_sentiment + (positive_count - negative_count) * 0.1
        
        return max(-1, min(1, final_sentiment))  # ØªÙ‚ÙŠÙŠØ¯ Ø¨ÙŠÙ† -1 Ùˆ 1
    
    def _extract_numbers(self, doc):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ©"""
        numbers = []
        
        for token in doc:
            if token.like_num or '%' in token.text:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚
                context = []
                for i in range(max(0, token.i - 3), min(len(doc), token.i + 3)):
                    context.append(doc[i].text)
                
                numbers.append({
                    'value': token.text,
                    'context': ' '.join(context)
                })
        
        return numbers
    
    def _calculate_impact_score(self, event_type, sentiment):
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ£Ø«ÙŠØ±"""
        base_impact = self.event_patterns.get(event_type, {}).get('impact_multiplier', 1)
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        if event_type in ['interest_rate', 'inflation']:
            # Ù„Ù„Ø°Ù‡Ø¨: Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø³Ù„Ø¨ÙŠØŒ Ø§Ù†Ø®ÙØ§Ø¶Ù‡Ø§ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
            impact = base_impact * (-sentiment)
        else:
            impact = base_impact * sentiment
        
        return round(impact, 2)
    
    def _analyze_events_impact(self, events):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ù„Ø£Ø­Ø¯Ø§Ø«"""
        if not events:
            return {
                'events': [],
                'total_impact': 0,
                'dominant_theme': None,
                'recommendation': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø­Ø¯Ø§Ø« Ù…Ø¤Ø«Ø±Ø©'
            }
        
        # ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø­Ø¯Ø«
        event_groups = {}
        for event in events:
            event_type = event['type']
            if event_type not in event_groups:
                event_groups[event_type] = []
            event_groups[event_type].append(event)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        total_impact = sum(event['impact_score'] for event in events)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†
        dominant_theme = max(event_groups.keys(), 
                           key=lambda k: sum(e['impact_score'] for e in event_groups[k]))
        
        # Ø§Ù„ØªÙˆØµÙŠØ©
        if total_impact > 5:
            recommendation = "Ø£Ø®Ø¨Ø§Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ø°Ù‡Ø¨"
        elif total_impact > 2:
            recommendation = "Ø£Ø®Ø¨Ø§Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ù„Ù„Ø°Ù‡Ø¨"
        elif total_impact < -5:
            recommendation = "Ø£Ø®Ø¨Ø§Ø± Ø³Ù„Ø¨ÙŠØ© Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ø°Ù‡Ø¨"
        elif total_impact < -2:
            recommendation = "Ø£Ø®Ø¨Ø§Ø± Ø³Ù„Ø¨ÙŠØ© Ù„Ù„Ø°Ù‡Ø¨"
        else:
            recommendation = "ØªØ£Ø«ÙŠØ± Ù…Ø­Ø§ÙŠØ¯ Ø£Ùˆ Ù…Ø®ØªÙ„Ø·"
        
        return {
            'events': events[:10],  # Ø£Ù‡Ù… 10 Ø£Ø­Ø¯Ø§Ø«
            'event_summary': {t: len(e) for t, e in event_groups.items()},
            'total_impact': round(total_impact, 2),
            'dominant_theme': dominant_theme,
            'recommendation': recommendation
        }
    
    async def fetch_news_async(self):
        """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
        keywords = [
            '"gold price"',
            '"federal reserve"',
            '"interest rates"',
            '"inflation data"',
            '"XAU/USD"'
        ]
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for keyword in keywords:
                url = f"https://newsapi.org/v2/everything?q={keyword}&language=en&sortBy=publishedAt&pageSize=20&apiKey={self.api_key}"
                tasks.append(self._fetch_url(session, url))
            
            results = await asyncio.gather(*tasks)
            
        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        all_articles = []
        for result in results:
            if result and 'articles' in result:
                all_articles.extend(result['articles'])
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article.get('url') not in seen_urls:
                seen_urls.add(article.get('url'))
                unique_articles.append(article)
        
        return unique_articles
    
    async def _fetch_url(self, session, url):
        """Ø¬Ù„Ø¨ URL ÙˆØ§Ø­Ø¯"""
        try:
            async with session.get(url, timeout=10) as response:
                return await response.json()
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {e}")
            return None

class ProfessionalBacktester:
    """Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± Ø®Ù„ÙÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ"""
    
    class GoldStrategy(bt.Strategy):
        params = (
            ('analyzer', None),
            ('risk_percent', 0.02),
        )
        
        def __init__(self):
            self.order = None
            self.buyprice = None
            self.buycomm = None
            self.trades = []
            
        def next(self):
            if self.order:
                return
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            current_data = self._prepare_current_data()
            signal = self.params.analyzer.generate_signal_for_backtest(current_data)
            
            if not self.position:
                # Ù…Ù†Ø·Ù‚ Ø§Ù„Ø´Ø±Ø§Ø¡
                if signal['action'] in ['Strong Buy', 'Buy']:
                    # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ø±ÙƒØ²
                    size = self._calculate_position_size(signal['confidence'], self.data.close[0])
                    if size > 0:
                        self.order = self.buy(size=size)
                    
            else:
                # Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¨ÙŠØ¹
                if signal['action'] in ['Strong Sell', 'Sell']:
                    self.order = self.sell()
                    
                # ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙˆØ¬Ù†ÙŠ Ø§Ù„Ø£Ø±Ø¨Ø§Ø­
                elif self.position.size > 0:
                    current_price = self.data.close[0]
                    entry_price = self.position.price
                    
                    # ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø©
                    if current_price < entry_price * 0.98:  # 2% stop loss
                        self.order = self.sell()
                        
                    # Ø¬Ù†ÙŠ Ø§Ù„Ø£Ø±Ø¨Ø§Ø­
                    elif current_price > entry_price * 1.05:  # 5% take profit
                        self.order = self.sell()
        
        def _prepare_current_data(self):
            """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„"""
            # Ù‡Ù†Ø§ ÙŠØªÙ… ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­ÙŠ
            return {
                'close': self.data.close[0],
                'open': self.data.open[0],
                'high': self.data.high[0],
                'low': self.data.low[0],
                'volume': self.data.volume[0],
                # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            }
        
        def _calculate_position_size(self, confidence, price):
            """âœ… ØªØµØ­ÙŠØ­: Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ø±ÙƒØ² (Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ø³Ø¹Ø±"""
            if price <= 0:
                return 0 # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±
                
            cash_to_risk = self.broker.getcash() * self.params.risk_percent
            base_size = cash_to_risk / price # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª
            
            if confidence == 'Very High':
                return base_size * 2
            elif confidence == 'High':
                return base_size * 1.5
            else:
                return base_size
        
        def notify_order(self, order):
            if order.status in [order.Submitted, order.Accepted]:
                return
                
            if order.status in [order.Completed]:
                if order.isbuy():
                    self.buyprice = order.executed.price
                    self.buycomm = order.executed.comm
                else:
                    profit = (order.executed.price - self.buyprice) * order.executed.size
                    self.trades.append({
                        'profit': profit,
                        'return': (order.executed.price - self.buyprice) / self.buyprice
                    })
                    
            self.order = None
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
        
    def run_backtest(self, data, initial_cash=10000):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ"""
        print("ğŸ”„ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ backtrader
        cerebro = bt.Cerebro()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_feed = bt.feeds.PandasData(dataname=data)
        cerebro.adddata(data_feed)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
        cerebro.addstrategy(self.GoldStrategy, analyzer=self.analyzer)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙˆØ³ÙŠØ·
        cerebro.broker.setcash(initial_cash)
        cerebro.broker.setcommission(commission=0.001)  # 0.1% Ø¹Ù…ÙˆÙ„Ø©
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­Ù„Ù„ÙŠÙ†
        cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')
        cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')
        cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')
        cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trades')
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        results = cerebro.run()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        strat = results[0]
        
        # Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
        sharpe = strat.analyzers.sharpe.get_analysis()
        drawdown = strat.analyzers.drawdown.get_analysis()
        returns = strat.analyzers.returns.get_analysis()
        trades = strat.analyzers.trades.get_analysis()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        final_value = cerebro.broker.getvalue()
        total_return = (final_value - initial_cash) / initial_cash * 100
        
        backtest_results = {
            'initial_capital': initial_cash,
            'final_value': round(final_value, 2),
            'total_return': round(total_return, 2),
            'sharpe_ratio': round(sharpe.get('sharperatio', 0), 3),
            'max_drawdown': round(drawdown.get('max', {}).get('drawdown', 0), 2),
            'total_trades': trades.get('total', {}).get('total', 0),
            'winning_trades': trades.get('won', {}).get('total', 0),
            'losing_trades': trades.get('lost', {}).get('total', 0),
            'win_rate': round(trades.get('won', {}).get('total', 0) / max(trades.get('total', {}).get('total', 1), 1) * 100, 2),
            'avg_trade_return': round(returns.get('rtot', 0) / max(trades.get('total', {}).get('total', 1), 1), 4),
            'best_trade': round(trades.get('won', {}).get('pnl', {}).get('max', 0), 2),
            'worst_trade': round(trades.get('lost', {}).get('pnl', {}).get('max', 0), 2),
            'avg_win': round(trades.get('won', {}).get('pnl', {}).get('average', 0), 2),
            'avg_loss': round(trades.get('lost', {}).get('pnl', {}).get('average', 0), 2),
            'profit_factor': self._calculate_profit_factor(trades),
            'recovery_factor': self._calculate_recovery_factor(total_return, drawdown),
            'risk_reward_ratio': self._calculate_risk_reward(trades)
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„
        self._generate_backtest_report(backtest_results)
        
        return backtest_results
    
    def _calculate_profit_factor(self, trades):
        """Ø­Ø³Ø§Ø¨ Ø¹Ø§Ù…Ù„ Ø§Ù„Ø±Ø¨Ø­"""
        try:
            gross_profit = abs(trades.get('won', {}).get('pnl', {}).get('total', 0))
            gross_loss = abs(trades.get('lost', {}).get('pnl', {}).get('total', 0))
            
            if gross_loss > 0:
                return round(gross_profit / gross_loss, 2)
            return 0
        except:
            return 0
    
    def _calculate_recovery_factor(self, total_return, drawdown):
        """Ø­Ø³Ø§Ø¨ Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ø³ØªØ±Ø¯Ø§Ø¯"""
        try:
            max_dd = abs(drawdown.get('max', {}).get('drawdown', 1))
            if max_dd > 0:
                return round(total_return / max_dd, 2)
            return 0
        except:
            return 0
    
    def _calculate_risk_reward(self, trades):
        """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©"""
        try:
            avg_win = abs(trades.get('won', {}).get('pnl', {}).get('average', 0))
            avg_loss = abs(trades.get('lost', {}).get('pnl', {}).get('average', 1))
            
            if avg_loss > 0:
                return round(avg_win / avg_loss, 2)
            return 0
        except:
            return 0
    
    def _generate_backtest_report(self, results):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ"""
        print("\n" + "="*60)
        print("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ")
        print("="*60)
        
        print(f"\nğŸ’° Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø§Ù„ÙŠ:")
        print(f"  â€¢ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ø§Ù„Ø£ÙˆÙ„ÙŠ: ${results['initial_capital']:,}")
        print(f"  â€¢ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: ${results['final_value']:,}")
        print(f"  â€¢ Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {results['total_return']}%")
        print(f"  â€¢ Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨: {results['sharpe_ratio']}")
        print(f"  â€¢ Ø£Ù‚ØµÙ‰ Ø§Ù†Ø®ÙØ§Ø¶: {results['max_drawdown']}%")
        
        print(f"\nğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„:")
        print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙÙ‚Ø§Øª: {results['total_trades']}")
        print(f"  â€¢ Ø§Ù„ØµÙÙ‚Ø§Øª Ø§Ù„Ø±Ø§Ø¨Ø­Ø©: {results['winning_trades']}")
        print(f"  â€¢ Ø§Ù„ØµÙÙ‚Ø§Øª Ø§Ù„Ø®Ø§Ø³Ø±Ø©: {results['losing_trades']}")
        print(f"  â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙÙˆØ²: {results['win_rate']}%")
        
        print(f"\nğŸ’µ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø±Ø¨Ø§Ø­ ÙˆØ§Ù„Ø®Ø³Ø§Ø¦Ø±:")
        print(f"  â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø±Ø¨Ø­: ${results['avg_win']}")
        print(f"  â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø³Ø§Ø±Ø©: ${results['avg_loss']}")
        print(f"  â€¢ Ø£ÙØ¶Ù„ ØµÙÙ‚Ø©: ${results['best_trade']}")
        print(f"  â€¢ Ø£Ø³ÙˆØ£ ØµÙÙ‚Ø©: ${results['worst_trade']}")
        print(f"  â€¢ Ø¹Ø§Ù…Ù„ Ø§Ù„Ø±Ø¨Ø­: {results['profit_factor']}")
        print(f"  â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©/Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {results['risk_reward_ratio']}")
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
        print(f"\nğŸ¯ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:")
        if results['sharpe_ratio'] > 2:
            print("  âœ… Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø² - Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨ Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹")
        elif results['sharpe_ratio'] > 1:
            print("  âœ… Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯ - Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨ Ø¬ÙŠØ¯Ø©")
        elif results['sharpe_ratio'] > 0.5:
            print("  âš ï¸ Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø¨ÙˆÙ„ - ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")
        else:
            print("  âŒ Ø£Ø¯Ø§Ø¡ Ø¶Ø¹ÙŠÙ - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø©")
        
        if results['win_rate'] > 60:
            print("  âœ… Ù…Ø¹Ø¯Ù„ ÙÙˆØ² Ù…Ù…ØªØ§Ø²")
        elif results['win_rate'] > 50:
            print("  âœ… Ù…Ø¹Ø¯Ù„ ÙÙˆØ² Ø¬ÙŠØ¯")
        else:
            print("  âš ï¸ Ù…Ø¹Ø¯Ù„ ÙÙˆØ² ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")
        
        if results['profit_factor'] > 2:
            print("  âœ… Ø¹Ø§Ù…Ù„ Ø±Ø¨Ø­ Ù…Ù…ØªØ§Ø²")
        elif results['profit_factor'] > 1.5:
            print("  âœ… Ø¹Ø§Ù…Ù„ Ø±Ø¨Ø­ Ø¬ÙŠØ¯")
        else:
            print("  âš ï¸ Ø¹Ø§Ù…Ù„ Ø±Ø¨Ø­ Ø¶Ø¹ÙŠÙ")
        
        print("="*60)

class DatabaseManager:
    """Ù…Ø¯ÙŠØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©"""
    
    def __init__(self, db_path="analysis_history.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                analysis_date DATE,
                gold_price REAL,
                signal TEXT,
                confidence TEXT,
                total_score REAL,
                component_scores TEXT,
                technical_indicators TEXT,
                volume_analysis TEXT,
                correlations TEXT,
                economic_score REAL,
                news_sentiment TEXT,
                mtf_coherence REAL,
                ml_probability REAL,
                price_after_1d REAL,
                price_after_5d REAL,
                price_after_10d REAL,
                price_change_1d REAL,
                price_change_5d REAL,
                price_change_10d REAL,
                signal_success BOOLEAN
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date DATE,
                metric_name TEXT,
                metric_value REAL,
                details TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def save_analysis(self, analysis_data):
        """Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            gold_analysis = analysis_data.get('gold_analysis', {})
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† gold_analysis Ù„ÙŠØ³ Ø®Ø·Ø£Ù‹
            if not gold_analysis or 'error' in gold_analysis:
                print("âš ï¸ ØªÙ… ØªØ®Ø·ÙŠ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø³Ø¨Ø¨ ÙˆØ¬ÙˆØ¯ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
                return

            cursor.execute('''
                INSERT INTO analysis_history (
                    analysis_date, gold_price, signal, confidence, total_score,
                    component_scores, technical_indicators, volume_analysis,
                    correlations, economic_score, news_sentiment, mtf_coherence,
                    ml_probability
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().date(),
                gold_analysis.get('current_price'),
                gold_analysis.get('signal'),
                gold_analysis.get('confidence'),
                gold_analysis.get('total_score'),
                json.dumps(gold_analysis.get('component_scores', {})),
                json.dumps(gold_analysis.get('technical_summary', {})),
                json.dumps(analysis_data.get('volume_analysis', {})),
                json.dumps(analysis_data.get('market_correlations', {}).get('correlations', {})),
                analysis_data.get('economic_data', {}).get('score', 0),
                analysis_data.get('news_analysis', {}).get('summary', {}).get('overall_sentiment'),
                analysis_data.get('mtf_analysis', {}).get('coherence_score', 0),
                gold_analysis.get('ml_prediction', {}).get('probability', 0)
            ))
            
            conn.commit()
            print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
            conn.rollback()
        finally:
            conn.close()
    
    def update_future_prices(self):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ ØªØ­Ø¯ÙŠØ«
            cursor.execute('''
                SELECT id, analysis_date, gold_price 
                FROM analysis_history 
                WHERE price_after_10d IS NULL 
                AND analysis_date <= date('now', '-10 days')
            ''')
            
            records = cursor.fetchall()
            
            for record_id, analysis_date, original_price in records:
                # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
                future_prices = self._get_future_prices(analysis_date)
                
                if future_prices:
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
                    changes = {
                        '1d': ((future_prices.get('1d', original_price) - original_price) / original_price * 100),
                        '5d': ((future_prices.get('5d', original_price) - original_price) / original_price * 100),
                        '10d': ((future_prices.get('10d', original_price) - original_price) / original_price * 100)
                    }
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
                    signal_success = changes['5d'] > 1.0  # Ù†Ø¬Ø§Ø­ Ø¥Ø°Ø§ Ø§Ø±ØªÙØ¹ Ø£ÙƒØ«Ø± Ù…Ù† 1%
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„
                    cursor.execute('''
                        UPDATE analysis_history 
                        SET price_after_1d = ?, price_after_5d = ?, price_after_10d = ?,
                            price_change_1d = ?, price_change_5d = ?, price_change_10d = ?,
                            signal_success = ?
                        WHERE id = ?
                    ''', (
                        future_prices.get('1d'), future_prices.get('5d'), future_prices.get('10d'),
                        changes['1d'], changes['5d'], changes['10d'],
                        signal_success, record_id
                    ))
            
            conn.commit()
            if len(records) > 0:
                print(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« {len(records)} Ø³Ø¬Ù„ Ø¨Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©: {e}")
            conn.rollback()
        finally:
            conn.close()
    
    def _get_future_prices(self, analysis_date):
        """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù„ØªØ§Ø±ÙŠØ® Ù…Ø¹ÙŠÙ†"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®
            if isinstance(analysis_date, str):
                analysis_date = datetime.strptime(analysis_date, '%Y-%m-%d').date()
            
            # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°Ù‡Ø¨
            end_date = analysis_date + timedelta(days=15)
            data = yf.download('GC=F', start=analysis_date, end=end_date, progress=False)
            
            if data.empty:
                return None
            
            prices = {}
            
            # Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø¨Ø¹Ø¯ 1ØŒ 5ØŒ 10 Ø£ÙŠØ§Ù…
            for days in [1, 5, 10]:
                target_date = analysis_date + timedelta(days=days)
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ ØªØ§Ø±ÙŠØ® Ù…ØªØ§Ø­
                for i in range(5):  # Ù…Ø­Ø§ÙˆÙ„Ø© 5 Ø£ÙŠØ§Ù… Ø¥Ø¶Ø§ÙÙŠØ©
                    check_date = target_date + timedelta(days=i)
                    if check_date in data.index:
                        prices[f'{days}d'] = data.loc[check_date, 'Close']
                        break
            
            return prices
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©: {e}")
            return None
    
    def get_training_data(self, min_records=100):
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
        conn = sqlite3.connect(self.db_path)
        
        query = '''
            SELECT * FROM analysis_history 
            WHERE signal_success IS NOT NULL 
            ORDER BY analysis_date DESC 
            LIMIT ?
        '''
        
        df = pd.read_sql_query(query, conn, params=(min_records * 2,))
        conn.close()
        
        if len(df) < min_records:
            print(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨: {len(df)} Ø³Ø¬Ù„ ÙÙ‚Ø·")
            return None
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª JSON
        training_data = []
        for _, row in df.iterrows():
            record = {
                'analysis': {
                    'gold_analysis': {
                        'current_price': row['gold_price'],
                        'signal': row['signal'],
                        'confidence': row['confidence'],
                        'total_score': row['total_score'],
                        'component_scores': json.loads(row['component_scores'] or '{}'),
                        'technical_summary': json.loads(row['technical_indicators'] or '{}')
                    },
                    'volume_analysis': json.loads(row['volume_analysis'] or '{}'),
                    'market_correlations': {
                        'correlations': json.loads(row['correlations'] or '{}')
                    },
                    'economic_data': {
                        'score': row['economic_score']
                    }
                },
                'price_change_5d': row['price_change_5d'],
                'signal_success': row['signal_success']
            }
            training_data.append(record)
        
        return training_data

class ProfessionalGoldAnalyzerV3:
    """Ø§Ù„Ø¥ØµØ¯Ø§Ø± 3.0 Ù…Ù† Ù…Ø­Ù„Ù„ Ø§Ù„Ø°Ù‡Ø¨ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ"""
    
    def __init__(self):
        # Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.symbols = {
            'gold': 'GC=F', 'gold_etf': 'GLD', 'dxy': 'DX-Y.NYB',
            'vix': '^VIX', 'treasury': '^TNX', 'oil': 'CL=F',
            'spy': 'SPY', 'usdeur': 'EURUSD=X', 'silver': 'SI=F'
        }
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        self.ml_predictor = MLPredictor()
        self.mtf_analyzer = MultiTimeframeAnalyzer()
        self.news_analyzer = AdvancedNewsAnalyzer(os.getenv("NEWS_API_KEY"))
        self.db_manager = DatabaseManager()
        self.backtester = ProfessionalBacktester(self)
        
        # APIs
        self.news_api_key = os.getenv("NEWS_API_KEY")
        self.fred_api_key = os.getenv("FRED_API_KEY")
    
    def fetch_multi_timeframe_data(self):
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        print("ğŸ“Š Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ©...")
        try:
            # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
            # âœ… ØªØµØ­ÙŠØ­: Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ
            daily_data = yf.download(list(self.symbols.values()), 
                                    period="3y", interval="1d", 
                                    group_by='ticker', progress=False)
            
            # Ø¨ÙŠØ§Ù†Ø§Øª 4 Ø³Ø§Ø¹Ø§Øª
            hourly_data = yf.download(self.symbols['gold'], 
                                     period="1mo", interval="1h", 
                                     progress=False)
            
            # Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ø¨ÙˆØ¹ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰
            weekly_data = yf.download(self.symbols['gold'], 
                                     period="2y", interval="1wk", 
                                     progress=False)
            
            if daily_data.empty: 
                raise ValueError("ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
            return {
                'daily': daily_data, 
                'hourly': hourly_data,
                'weekly': weekly_data
            }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return None
    
    def extract_gold_data(self, market_data):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°Ù‡Ø¨"""
        print("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°Ù‡Ø¨...")
        try:
            daily_data = market_data['daily']
            gold_symbol = self.symbols['gold']
            
            if not (gold_symbol in daily_data.columns.levels[0] and 
                   not daily_data[gold_symbol].dropna().empty):
                gold_symbol = self.symbols['gold_etf']
                if not (gold_symbol in daily_data.columns.levels[0] and 
                       not daily_data[gold_symbol].dropna().empty):
                    raise ValueError("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø°Ù‡Ø¨")
            
            gold_daily = daily_data[gold_symbol].copy()
            gold_daily.dropna(subset=['Close'], inplace=True)
            
            if len(gold_daily) < 200: 
                raise ValueError("Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©")
            
            print(f"âœ… Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ…ÙŠØ© Ù†Ø¸ÙŠÙØ©: {len(gold_daily)} ÙŠÙˆÙ…")
            return gold_daily
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°Ù‡Ø¨: {e}")
            return None
    
    def calculate_professional_indicators(self, gold_data):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©"
