#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ProfessionalGoldAnalyzer - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©
Ù…ÙŠØ²Ø§Øª Ù…Ø¶Ø§ÙØ©:
- Ù…ØµØ¯Ø± Ø³Ø¹Ø± Ø§Ù„Ø£ÙˆÙ†ØµØ©: XAUUSD=X
- ØªØµÙÙŠØ© Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (zero-shot) Ù‚Ø¨Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
- ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø¯ÙØ¹ÙŠ (batch) Ø¹Ø¨Ø± FinBERT Ø¥Ù† Ø£Ù…ÙƒÙ†
- ØªØ·Ø¨ÙŠØ¹ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø¯Ù…Ø¬
- Ø­ÙØ¸ Ø¥Ø´Ø§Ø±Ø§Øª ÙŠÙˆÙ…ÙŠØ© Ùˆ backtest Ø¨Ø³ÙŠØ·
- Ù…Ø®Ø±Ø¬Ø§Øª: gold_analysis.json, historical_signals.csv, backtest_report.json
"""
import os
import json
from datetime import datetime, timedelta
import requests
import warnings

import numpy as np
import pandas as pd
import yfinance as yf
import pandas_ta as ta

# transformers ÙŠØ³ØªØ®Ø¯Ù… Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ (zero-shot + finbert)
from transformers import pipeline, Pipeline, AutoTokenizer, AutoModelForSequenceClassification

warnings.filterwarnings("ignore")


class ProfessionalGoldAnalyzerV2:
    def __init__(self,
                 lookback_days=365,
                 news_days=2,
                 news_api_key=None,
                 save_path=".",
                 batch_size=16):
        self.symbols = {
            'gold': 'XAUUSD=X',   # Ø³Ø¹Ø± Ø§Ù„Ø£ÙˆÙ†ØµØ© Ø¨Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±
            'dxy': 'DX-Y.NYB',
            'vix': '^VIX',
            'treasury': '^TNX',
            'oil': 'CL=F',
            'spy': 'SPY'
        }
        self.lookback_days = lookback_days
        self.news_days = news_days
        self.news_api_key = news_api_key or os.getenv("NEWS_API_KEY")
        self.save_path = save_path
        self.batch_size = batch_size

        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: zero-shot Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØµÙ„Ø©ØŒ Ùˆ finbert Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        self.zero_shot = None
        self.sentiment = None
        self._load_models()

    def _load_models(self):
        try:
            print("ğŸ§  ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ zero-shot (NLI) Ù„ØªØµÙÙŠØ© Ø§Ù„Ø£Ø®Ø¨Ø§Ø±...")
            # ÙŠØ³ØªØ®Ø¯Ù… MNLI-based model Ø¹Ø¨Ø± pipeline zero-shot-classification
            self.zero_shot = pipeline("zero-shot-classification",
                                      model="facebook/bart-large-mnli")
            print("âœ… zero-shot Ø¬Ø§Ù‡Ø².")
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ zero-shot: {e}. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ„ØªØ±Ø© ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø¯ÙŠÙ„Ø©.")
            self.zero_shot = None

        try:
            print("ğŸ§  ØªØ­Ù…ÙŠÙ„ FinBERT Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø§Ù„ÙŠØ© (batch)...")
            # ProsusAI/finbert Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…ØªØ§Ø­Ù‹Ø§Ø› ÙƒØ¨Ø¯ÙŠÙ„ Ù†Ø³ØªØ®Ø¯Ù… model Ø¹Ø§Ù… Ø¥Ù† ÙØ´Ù„
            self.sentiment = pipeline("sentiment-analysis",
                                      model="ProsusAI/finbert")
            print("âœ… FinBERT Ø¬Ø§Ù‡Ø².")
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ FinBERT: {e}. Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø§Ù… Ù„Ù„ØªØ­Ù„ÙŠÙ„.")
            try:
                self.sentiment = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
                print("âœ… Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„ Ø¬Ø§Ù‡Ø².")
            except Exception as e2:
                print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù…Ø´Ø§Ø¹Ø±: {e2}")
                self.sentiment = None

    def fetch_market_data(self):
        print("\nğŸ“Š Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ù…Ù† Yahoo Finance...")
        try:
            symbols = list(self.symbols.values())
            data = yf.download(symbols, period=f"{self.lookback_days}d", interval="1d", progress=False)
            if data.empty:
                raise ValueError("Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ© Ù…Ù† Yahoo Finance.")
            print(f"... Ù†Ø¬Ø­ Ø¬Ù„Ø¨ {len(data)} ØµÙÙˆÙ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
            return data
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚: {e}")
            return None

    def fetch_news(self):
        """
        Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† NewsAPI (Ø¥Ù† ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§).
        ØªØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‚Ø§Ù„Ø§Øª (title, description, source, publishedAt).
        """
        print("\nğŸ“° Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† NewsAPI...")
        if not self.news_api_key:
            print("âš ï¸ Ù…ÙØªØ§Ø­ NewsAPI ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±.")
            return []

        query = ('gold OR XAU OR bullion OR "precious metal" OR "gold price" '
                 'OR "interest rate" OR fed OR inflation OR CPI OR NFP OR geopolitical')
        from_date = (datetime.utcnow() - timedelta(days=self.news_days)).date()
        url = ("https://newsapi.org/v2/everything"
               f"?q={requests.utils.quote(query)}&language=en&sortBy=publishedAt&pageSize=100"
               f"&from={from_date}&apiKey={self.news_api_key}")

        try:
            res = requests.get(url, timeout=20)
            res.raise_for_status()
            articles = res.json().get("articles", [])
            simple = []
            for a in articles:
                simple.append({
                    "title": a.get("title"),
                    "description": a.get("description"),
                    "content": a.get("content"),
                    "source": a.get("source", {}).get("name"),
                    "publishedAt": a.get("publishedAt")
                })
            print(f"... ØªÙ… Ø¬Ù„Ø¨ {len(simple)} Ù…Ù‚Ø§Ù„Ø© Ù…Ù† NewsAPI.")
            return simple
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {e}")
            return []

    def filter_relevant_articles(self, articles):
        """
        ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… zero-shot classification Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ù…ØªØ§Ø­Ù‹Ø§ØŒ
        ÙˆØ¥Ù„Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø³ÙŠØ·Ø©.
        """
        print("\nğŸ” ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ø°Ù‡Ø¨...")
        if not articles:
            return []

        candidates = []
        labels = ["gold", "economy", "geopolitics", "other"]
        for art in articles:
            text = ((art.get("title") or "") + " " + (art.get("description") or "")).strip()
            if not text:
                continue
            is_relevant = False
            score = 0
            if self.zero_shot:
                try:
                    out = self.zero_shot(text, candidate_labels=labels, multi_label=False)
                    # Ù†Ø¹ØªØ¨Ø±Ù‡Ø§ Ù…Ø±ØªØ¨Ø·Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙØ¦Ø© "gold" Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø¹ØªØ¨Ø©
                    if out and out.get("labels"):
                        if out["labels"][0] == "gold" and out["scores"][0] >= 0.45:
                            is_relevant = True
                            score = float(out["scores"][0])
                except Exception:
                    # ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ Ù†Ù…ÙˆØ°Ø¬ NLI Ù†ØªØ®Ø·Ù‰ Ù„Ù„Ø§Ø­ØªÙŠØ§Ø·
                    is_relevant = False

            if not self.zero_shot:
                # ÙÙ„ØªØ±Ø© ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø«Ø§Ù†ÙˆÙŠØ©
                kw = ["gold", "xau", "bullion", "precious metal", "troy ounce", "spot gold"]
                lower = text.lower()
                if any(k in lower for k in kw):
                    is_relevant = True
                    score = 0.5

            if is_relevant:
                art["_relevance_score"] = score
                candidates.append(art)

        print(f"... {len(candidates)} Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ÙÙ„ØªØ±Ø©.")
        return sorted(candidates, key=lambda x: x.get("_relevance_score", 0), reverse=True)

    def analyze_sentiment_batch(self, articles):
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¯ÙØ¹ÙŠÙ‹Ø§: Ø§Ø³ØªÙ„Ø§Ù… Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø±Ø´Ø­Ø© ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†ØªÙŠØ¬Ø©.
        Ù†Ø­ÙˆÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ù„Ù‰ Ù…Ù‚ÙŠØ§Ø³ Ù…ÙˆØ­Ø¯: pos -> +score, neg -> -score, neutral -> 0
        """
        print("\nğŸ§¾ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (Ø¯ÙØ¹ÙŠ)...")
        if not articles or not self.sentiment:
            print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª/Ø£Ùˆ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø´Ø§Ø¹Ø± ØºÙŠØ± Ù…ØªØ§Ø­ â€” Ø³ÙŠØªÙ… Ø¥Ø±Ø¬Ø§Ø¹ 0.")
            return {"status": "skipped", "news_score": 0, "headlines": []}

        texts = []
        for a in articles:
            txt = (a.get("description") or a.get("title") or "")
            texts.append(txt)

        results = []
        try:
            # batch call support
            for i in range(0, len(texts), self.batch_size):
                batch = texts[i:i + self.batch_size]
                res = self.sentiment(batch)
                results.extend(res)
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¯ÙØ¹ÙŠÙ‹Ø§: {e}")
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± ÙˆØ§Ø­Ø¯ ÙˆØ§Ø­Ø¯
            results = []
            for t in texts:
                try:
                    out = self.sentiment(t)[0]
                    results.append(out)
                except Exception:
                    results.append({"label": "NEUTRAL", "score": 0.0})

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ØµÙ‚Ø§Øª Ø¥Ù„Ù‰ Ù…Ù‚ÙŠØ§Ø³ Ù…ÙˆØ­Ø¯
        numeric = []
        headlines = []
        for art, r in zip(articles, results):
            lbl = r.get("label", "").lower()
            sc = float(r.get("score", 0.0))
            val = 0.0
            # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù…Ø³Ù…ÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù‚Ø¯ ØªÙƒÙˆÙ† "POSITIVE"/"NEGATIVE"/"NEUTRAL" Ø£Ùˆ "LABEL_0"...
            if "pos" in lbl or lbl == "positive":
                val = sc
            elif "neg" in lbl or lbl == "negative":
                val = -sc
            else:
                val = 0.0
            numeric.append(val)
            headlines.append({
                "title": art.get("title"),
                "source": art.get("source"),
                "relevance": art.get("_relevance_score", 0),
                "sentiment": round(val, 4)
            })

        # Ù†Ø­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ù‘Ø­ Ø­Ø³Ø¨ relevance
        relevances = np.array([a.get("_relevance_score", 0.5) for a in articles], dtype=float)
        vals = np.array(numeric, dtype=float)
        if relevances.sum() == 0:
            news_score = float(vals.mean()) if len(vals) else 0.0
        else:
            news_score = float(np.sum(vals * relevances) / (relevances.sum()))

        news_score = round(news_score, 4)
        print(f"... news_score = {news_score} (Ù…Ø±Ø¬Ù‘Ø­ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø©)")
        return {"status": "success", "news_score": news_score, "headlines": headlines[:10]}

    def compute_indicators(self, gold_df):
        """
        Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas_ta
        """
        ta_strategy = ta.Strategy(name="FullAnalysis", ta=[
            {"kind": "sma", "length": 50}, {"kind": "sma", "length": 200},
            {"kind": "rsi"}, {"kind": "macd"}, {"kind": "bbands"},
            {"kind": "atr"}, {"kind": "obv"}
        ])
        gold_df.ta.strategy(ta_strategy)
        return gold_df

    @staticmethod
    def normalize_component(val, min_val=-2, max_val=2):
        """
        ØªØ·Ø¨ÙŠØ¹ Ù‚ÙŠÙ…Ø© Ø¥Ù„Ù‰ Ù†Ø·Ø§Ù‚ [-1, 1]
        Ù†ÙØªØ±Ø¶ Ø£Ù† val ÙŠÙ‚Ø¹ Ø¶Ù…Ù† [min_val, max_val]
        """
        # clip to avoid extreme
        v = max(min(val, max_val), min_val)
        # scale to [-1,1]
        return 2 * (v - min_val) / (max_val - min_val) - 1

    def score_components(self, latest_row, market_data, news_score):
        """
        ØªÙˆÙ„ÙŠØ¯ Ù‚ÙŠÙ… Ù…Ø¨Ø¯Ø¦ÙŠØ© Ø«Ù… ØªØ·Ø¨ÙŠØ¹Ù‡Ø§:
        - trend_score: -1 (strong down) .. +1 (strong up)
        - momentum_score: -1 .. +1
        - correlation_score: -1 .. +1
        - news_score: Ù…ÙØªØ±Ø¶ ÙÙŠ [-1,1] Ø¨Ø§Ù„ÙØ¹Ù„ Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        """
        # Trend: Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø¹Ø± Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€ SMA200 Ùˆ SMA50
        trend_val = 0.0
        try:
            close = latest_row["Close"]
            sma200 = latest_row.get("SMA_200", np.nan)
            sma50 = latest_row.get("SMA_50", np.nan)
            if not np.isnan(sma200) and not np.isnan(sma50):
                # Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ SMA200 (Ù…Ù‚Ø³ÙˆÙ…Ø© Ø¹Ù„Ù‰ SMA200) Ù…Ø¶Ø±ÙˆØ¨Ø© Ø¨Ù…Ù‚ÙŠØ§Ø³ 2
                diff = (close - sma200) / sma200
                trend_val = diff * 10  # Ù…Ù‚ÙŠØ§Ø³ Ù…Ø¨Ø¯Ø¦ÙŠØŒ Ø³ÙŠØªÙ… ØªØ·Ø¨ÙŠØ¹Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§
        except Exception:
            trend_val = 0.0

        # Momentum: Ø§Ø³ØªØ®Ø¯Ø§Ù… MACD histogram Ø¥Ù† ÙˆØ¬Ø¯
        momentum_val = 0.0
        try:
            macd_hist = latest_row.get("MACDh_12_26_9", np.nan)
            if not np.isnan(macd_hist):
                momentum_val = macd_hist / max(abs(macd_hist) + 1e-9, 1) * 2
        except Exception:
            momentum_val = 0.0

        # Correlation: Ø§Ø±ØªØ¨Ø§Ø· Ø³Ø¹Ø± Ø§Ù„Ø°Ù‡Ø¨ Ù…Ø¹ DXY (Ø¹Ø§Ø¯Ø© Ø³Ø§Ù„Ø¨)
        correlation_val = 0.0
        try:
            gold_close = market_data[('Close', self.symbols['gold'])]
            dxy_close = market_data[('Close', self.symbols['dxy'])]
            corr = gold_close.corr(dxy_close)
            # Ù†Ø­ÙˆÙ‘Ù„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù„Ù…Ù‚ÙŠØ§Ø³ Ø­ÙŠØ« -1 => +1 (Ù„Ø£Ù† Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø³Ù„Ø¨ÙŠ Ù„Ù„Ø°Ù‡Ø¨ Ù…Ø¹ Ø§Ù„Ø¯ÙˆÙ„Ø§Ø± ÙŠØ¹Ø²Ø² Ø§Ù„Ø°Ù‡Ø¨)
            correlation_val = -corr if not np.isnan(corr) else 0.0
        except Exception:
            correlation_val = 0.0

        # ØªØ·Ø¨ÙŠØ¹ ÙƒÙ„ Ù‚ÙŠÙ…Ø© Ø¥Ù„Ù‰ [-1,1]
        trend_norm = self.normalize_component(trend_val, min_val=-2, max_val=2)
        momentum_norm = self.normalize_component(momentum_val, min_val=-2, max_val=2)
        correlation_norm = self.normalize_component(correlation_val, min_val=-1, max_val=1)
        news_norm = max(min(news_score, 1.0), -1.0)

        components = {
            "trend_score_raw": round(trend_val, 4),
            "momentum_score_raw": round(momentum_val, 4),
            "correlation_score_raw": round(correlation_val, 4),
            "trend_score": round(trend_norm, 4),
            "momentum_score": round(momentum_norm, 4),
            "correlation_score": round(correlation_norm, 4),
            "news_score": round(news_norm, 4)
        }

        return components

    def decide_signal(self, comps, thresholds=None):
        """
        Ù‚Ø±Ø§Ø± Ù†Ù‡Ø§Ø¦ÙŠ: Ù†Ø¬Ù…Ø¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ù‚ØµÙŠØ±Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¹Ø¯ÙŠÙ„.
        Ø§Ù„Ø£ÙˆØ²Ø§Ù†:
         - trend 40%
         - momentum 30%
         - correlation 20%
         - news 10%
        Ù†Ø­Ø³Ø¨ total âˆˆ [-1,1]. Ù†Ø­Ø¯Ø¯ Ø¥Ø´Ø§Ø±Ø©:
         - Buy Ø¥Ø°Ø§ >= 0.4
         - Sell Ø¥Ø°Ø§ <= -0.4
         - Else Hold
        """
        if thresholds is None:
            thresholds = {"buy": 0.4, "sell": -0.4}
        w = {"trend": 0.4, "momentum": 0.3, "correlation": 0.2, "news": 0.1}
        total = (comps["trend_score"] * w["trend"] +
                 comps["momentum_score"] * w["momentum"] +
                 comps["correlation_score"] * w["correlation"] +
                 comps["news_score"] * w["news"])
        if total >= thresholds["buy"]:
            sig = "Buy"
        elif total <= thresholds["sell"]:
            sig = "Sell"
        else:
            sig = "Hold"
        return round(total, 4), sig

    def save_json(self, filename, data):
        path = os.path.join(self.save_path, filename)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Ø­ÙØ¸: {path}")

    def run_full_analysis(self):
        market_data = self.fetch_market_data()
        if market_data is None:
            return {"status": "error", "error": "market fetch failed"}

        # Ø¬Ù‡Ø² DataFrame Ù„Ù„Ø°Ù‡Ø¨
        gold_ticker = self.symbols["gold"]
        gold_df = pd.DataFrame({
            "Open": market_data[("Open", gold_ticker)],
            "High": market_data[("High", gold_ticker)],
            "Low": market_data[("Low", gold_ticker)],
            "Close": market_data[("Close", gold_ticker)],
            "Volume": market_data[("Volume", gold_ticker)]
        }).dropna()

        if gold_df.empty:
            return {"status": "error", "error": "no gold data"}

        # Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ©
        gold_df = self.compute_indicators(gold_df)
        gold_df.dropna(inplace=True)

        # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆÙÙ„ØªØ±ØªÙ‡Ø§
        articles = self.fetch_news()
        relevant = self.filter_relevant_articles(articles)
        news_result = self.analyze_sentiment_batch(relevant)

        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ÙŠÙˆÙ… Ø§Ù„Ø£Ø®ÙŠØ±
        latest = gold_df.iloc[-1]
        comps = self.score_components(latest, market_data, news_result.get("news_score", 0))
        total_score, signal = self.decide_signal(comps)

        result = {
            "timestamp_utc": datetime.utcnow().isoformat(),
            "signal": signal,
            "total_score": total_score,
            "components": comps,
            "market_data": {
                "gold_price": float(round(latest["Close"], 4)),
                "dxy": float(round(market_data[('Close', self.symbols['dxy'])].iloc[-1], 4)),
                "vix": float(round(market_data[('Close', self.symbols['vix'])].iloc[-1], 4)) if ('Close', self.symbols['vix']) in market_data else None
            },
            "news_analysis": news_result
        }

        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„Ù€ backtest
        self.save_json("gold_analysis.json", result)

        # Ø­ÙØ¸ historical signals (append)
        signals_path = os.path.join(self.save_path, "historical_signals.csv")
        row = {
            "timestamp_utc": result["timestamp_utc"],
            "signal": signal,
            "total_score": total_score,
            "gold_price": result["market_data"]["gold_price"],
            "news_score": comps["news_score"],
            "trend_score": comps["trend_score"],
            "momentum_score": comps["momentum_score"],
            "correlation_score": comps["correlation_score"]
        }
        df_row = pd.DataFrame([row])
        if not os.path.exists(signals_path):
            df_row.to_csv(signals_path, index=False, encoding="utf-8")
        else:
            df_row.to_csv(signals_path, mode="a", index=False, header=False, encoding="utf-8")
        print(f"ğŸ’¾ ØªÙ… ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª: {signals_path}")

        return result

    def backtest_signals(self, signals_csv=None, price_series=None):
        """
        backtest Ø¨Ø³ÙŠØ·:
        - Ø¥Ø¯Ø®Ø§Ù„: Ù…Ù„Ù historical_signals.csv Ø£Ùˆ Ø³Ù„Ø³Ù„Ø© Ø³Ø¹Ø±ÙŠØ© (pandas.Series indexed by date)
        - Ø§Ø³ØªØ±Ø§ØªØ¬ÙŠØ©: Ø¹Ù†Ø¯ 'Buy' Ù†Ø¯Ø®Ù„ Ù…Ø±ÙƒØ² Ø·ÙˆÙŠÙ„ Ø¨ÙƒØ§Ù…Ù„ Ø§Ù„Ø±ØµÙŠØ¯ (position=1)ØŒ Ø¹Ù†Ø¯ 'Sell' Ù†Ø®Ø±Ø¬ (position=0).
          'Hold' ÙŠØ­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø³Ø§Ø¨Ù‚.
        - Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ù…Ø¦ÙˆÙŠØ© ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§.
        Ù…Ø®Ø±Ø¬Ø§Øª: ØªÙ‚Ø±ÙŠØ± Ø¨Ø³ÙŠØ· (total_return, CAGR, max_drawdown, trades)
        """
        print("\nğŸ“ˆ Ø¨Ø¯Ø¡ backtest Ø¨Ø³ÙŠØ·...")
        try:
            if signals_csv is None:
                signals_csv = os.path.join(self.save_path, "historical_signals.csv")
            if not os.path.exists(signals_csv):
                print("âš ï¸ Ù…Ù„Ù historical_signals.csv ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„ backtest.")
                return {"status": "error", "error": "no signals file"}

            sigs = pd.read_csv(signals_csv, parse_dates=["timestamp_utc"])
            sigs.sort_values("timestamp_utc", inplace=True)
            sigs.reset_index(drop=True, inplace=True)

            # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ³Ù„Ø³Ù„ Ø£Ø³Ø¹Ø§Ø± ØªØ§Ø±ÙŠØ®ÙŠ Ù…ØªÙˆØ§ÙÙ‚ØŒ Ù†Ø³ØªØ®Ø±Ø¬ Ù…Ù† Yahoo ØªØ§Ø±ÙŠØ® ÙŠØªØ±Ø§ÙˆØ­ Ø¨ÙŠÙ† Ø£ÙˆÙ„ ÙˆØ¢Ø®Ø± Ø¥Ø´Ø§Ø±Ø©
            start = sigs["timestamp_utc"].dt.date.min()
            end = sigs["timestamp_utc"].dt.date.max() + pd.Timedelta(days=1)
            prices = yf.download(self.symbols["gold"], start=start.isoformat(), end=end.isoformat(), interval="1d", progress=False)
            if prices.empty:
                return {"status": "error", "error": "no price series fetched for backtest"}

            close = prices["Close"].ffill().dropna()
            close.index = pd.to_datetime(close.index)

            # Ù†Ø¯Ù…Ø¬ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±: Ù„ÙƒÙ„ ÙŠÙˆÙ… Ø¥Ø´Ø§Ø±Ø© Ù†Ø¹ØªØ¨Ø±Ù‡Ø§ ØªÙØ·Ø¨Ù‘ÙÙ‚ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„ÙŠÙˆÙ… Ù†ÙØ³Ù‡ (could be improved)
            sigs["date"] = pd.to_datetime(sigs["timestamp_utc"]).dt.normalize()
            # ØªØ¨Ø³ÙŠØ·: Ù†Ø£Ø®Ø° Ø¢Ø®Ø± Ø¥Ø´Ø§Ø±Ø© Ù„ÙƒÙ„ ØªØ§Ø±ÙŠØ®
            daily_sig = sigs.groupby("date").last().reindex(close.index, method="ffill").fillna(method="ffill")
            # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ÙˆÙ‚ÙÙŠØ©: Buy -> 1, Sell -> 0, Hold -> previous
            position = []
            pos = 0
            for idx, row in daily_sig.iterrows():
                s = row["signal"]
                if s == "Buy":
                    pos = 1
                elif s == "Sell":
                    pos = 0
                # Hold leaves pos as is
                position.append(pos)
            pf = pd.DataFrame({"close": close, "position": position}, index=close.index)

            # Ø­Ø³Ø§Ø¨ Ø¹ÙˆØ§Ø¦Ø¯ ÙŠÙˆÙ…ÙŠØ©
            pf["pct_change"] = pf["close"].pct_change().fillna(0)
            pf["strategy_return"] = pf["position"].shift(1).fillna(0) * pf["pct_change"]  # position ì ìš© Ø¹Ù„Ù‰ Ø¹Ø§Ø¦Ø¯ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„ØªØ§Ù„ÙŠ
            pf["cum_return"] = (1 + pf["strategy_return"]).cumprod()
            pf["buyhold_cum"] = (1 + pf["pct_change"]).cumprod()

            total_return = float(pf["cum_return"].iloc[-1] - 1)
            bh_return = float(pf["buyhold_cum"].iloc[-1] - 1)

            # CAGR ØªÙ‚Ø±ÙŠØ¨ÙŠ
            days = (pf.index[-1] - pf.index[0]).days or 1
            years = days / 365.25
            cagr = (pf["cum_return"].iloc[-1]) ** (1 / years) - 1 if years > 0 else 0
            bh_cagr = (pf["buyhold_cum"].iloc[-1]) ** (1 / years) - 1 if years > 0 else 0

            # max drawdown
            roll_max = pf["cum_return"].cummax()
            drawdown = pf["cum_return"] / roll_max - 1
            max_dd = float(drawdown.min())

            trades = int(((daily_sig["signal"] == "Buy") | (daily_sig["signal"] == "Sell")).sum())

            report = {
                "period_start": str(pf.index[0].date()),
                "period_end": str(pf.index[-1].date()),
                "days": int(days),
                "total_return": round(total_return, 4),
                "cagr": round(cagr, 4),
                "buyhold_return": round(bh_return, 4),
                "buyhold_cagr": round(bh_cagr, 4),
                "max_drawdown": round(max_dd, 4),
                "trades": trades
            }

            # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            self.save_json("backtest_report.json", report)
            print("âœ… backtest Ù…ÙƒØªÙ…Ù„.")
            return {"status": "success", "report": report, "pf_head": pf.head(3).to_dict()}
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ backtest: {e}")
            return {"status": "error", "error": str(e)}


if __name__ == "__main__":
    analyzer = ProfessionalGoldAnalyzerV2(
        lookback_days=365,
        news_days=2,
        news_api_key=os.getenv("NEWS_API_KEY"),
        save_path=".",
        batch_size=16
    )
    res = analyzer.run_full_analysis()
    print("\n--- Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---")
    print(json.dumps(res, indent=2, ensure_ascii=False))

    # Ù†ÙØ° backtest ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ù† ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø³Ø¬Ù„ Ø¥Ø´Ø§Ø±Ø§Øª
    bt = analyzer.backtest_signals()
    print("\n--- Ù…Ù„Ø®Øµ backtest ---")
    print(json.dumps(bt, indent=2, ensure_ascii=False))